<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>k8s on Cactus theme example</title>
    <link>https://example.com/tags/k8s/</link>
    <description>Recent content in k8s on Cactus theme example</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>You</copyright>
    <lastBuildDate>Sat, 20 Jun 2020 12:44:22 +0800</lastBuildDate><atom:link href="https://example.com/tags/k8s/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>自己动手实现一个kubectl exec</title>
      <link>https://example.com/posts/kubectl-exec/</link>
      <pubDate>Sat, 20 Jun 2020 12:44:22 +0800</pubDate>
      
      <guid>https://example.com/posts/kubectl-exec/</guid>
      <description>在日常工作中kubectl exec 可以说是非常高频使用的，如果你想自己了解相关原理，不妨自己动手写一个。
知识储备：
  websocket 阮一峰这篇《WebSocket 教程- 阮一峰的网络日志》写的比较详进。
  kubectl exec 原理
https://itnext.io/how-it-works-kubectl-exec-e31325daa910
https://erkanerol.github.io/post/how-kubectl-exec-works/
如果你英文阅读能还可以，这两篇文章从原理方面介绍了exec是如何工作的。
  了解了以上知识之后，接下来我们就开始动手吧。
首先来初始化一下项目，这里使用go mod作为依赖管理工具。k8s的client-go对机器版本是有要求的，所以在初始化的时候最好去官方那边找一下可用的版本。如果遇到mod/k8s.io/client-go@v10.0.0+incompatible/kubernetes/scheme/register.go:22:2: unknown import path &amp;quot;k8s.io/api/admissionregistration /v1alpha1&amp;quot;: cannot find module providing package k8s.io/api/admissionregistration/v1alpha1
这种报错，可以尝试强制指定版本，这个也是从kubebuilder那里学到的。
1go mod init k8sdemo 2 3module k8sdemo 4 5go 1.13 6 7require ( 8 github.com/gorilla/websocket v1.4.2 9 golang.org/x/crypto v0.0.0-20190820162420-60c769a6c586 10 k8s.io/api v0.17.2 11 k8s.io/apimachinery v0.17.2 12 k8s.io/client-go v0.17.2 13) client-go的example目录也有相关对象的CURD示例，我们可以先从这里入手，先熟悉相关操作，可以看到首先从kuebconfig读取配置，然后初始化各种client的一个集合，最后创建了一个deployment实例。
1	config, err := clientcmd.BuildConfigFromFlags(&amp;#34;&amp;#34;, *kubeconfig) 2	if err !</description>
    </item>
    
    <item>
      <title>Containerd gvisor &amp; k8s</title>
      <link>https://example.com/posts/containerd-gvisor/</link>
      <pubDate>Mon, 25 Nov 2019 12:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/containerd-gvisor/</guid>
      <description>本文主要讨论如何在containerd中使用gvisor作为runtime，以及如何对接k8s。
安装containerd 这个就不用多说了，直接apt install containerd 即可，会安装一下几个可执行文件，如下所示
1/usr/bin/containerd 守护进程 2/usr/bin/containerd-shim 应该是负责处理容器内stdout &amp;amp; stderr 3/usr/bin/containerd-shim-runc-v1 应该是负责启动容器内的pid 1 4/usr/bin/containerd-stress 压力测试工具 5/usr/bin/ctr cli工具  吐槽一下 ctr可能没有docker的cli那么好用
 接下来我们来运行一个容器测试一下containerd：
 因为containerd默认的pause容器使用的是gcr.k8s.io/pause，如果网络有问题的话，就需要提前导入，或者在修改默认配置。 还要提醒一下，containerd有namespace的区别，默认是在default下面，这里不是k8s的namespace 。 除此之外还要配置CNI以供容器启动分配网络，可以使用下面的配置：  1mkdir -p /etc/cni/net.d 2cat &amp;gt;&amp;#39;/etc/cni/net.d&amp;#39;/10-containerd-net.conflist &amp;lt;&amp;lt;EOF 3{ 4 &amp;#34;cniVersion&amp;#34;: &amp;#34;0.3.1&amp;#34;, 5 &amp;#34;name&amp;#34;: &amp;#34;containerd-net&amp;#34;, 6 &amp;#34;plugins&amp;#34;: [ 7 { 8 &amp;#34;type&amp;#34;: &amp;#34;bridge&amp;#34;, 9 &amp;#34;bridge&amp;#34;: &amp;#34;cni1&amp;#34;, 10 &amp;#34;isGateway&amp;#34;: true, 11 &amp;#34;ipMasq&amp;#34;: true, 12 &amp;#34;promiscMode&amp;#34;: true, 13 &amp;#34;ipam&amp;#34;: { 14 &amp;#34;type&amp;#34;: &amp;#34;host-local&amp;#34;, 15 &amp;#34;subnet&amp;#34;: &amp;#34;10.</description>
    </item>
    
    <item>
      <title>kubectl 常用快捷键</title>
      <link>https://example.com/posts/kubectl/</link>
      <pubDate>Fri, 20 Sep 2019 22:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/kubectl/</guid>
      <description>查询类 1# custom-columns 2 3# 比如 查看role=sre 标签pod镜像，当然也可以用go的自定义输出 4kubectl -n kube-system get po -l tier=control-plane -o custom-columns=NAME:.metadata.name,image:.spec.containers[0].image 5NAME image 6etcd-ubuntu-bionic gcr.azk8s.cn/google_containers/etcd:3.3.15-0 7kube-apiserver-ubuntu-bionic gcr.azk8s.cn/google_containers/kube-apiserver:v1.16.2 8kube-controller-manager-ubuntu-bionic gcr.azk8s.cn/google_containers/kube-controller-manager:v1.16.2 9kube-scheduler-ubuntu-bionic gcr.azk8s.cn/google_containers/kube-scheduler:v1.16.2 10 11# go-template  12 13# range 嵌套 14# 列出所有容器使用的镜像名 15kubectl get pods -o go-template --template=&amp;#39;{{range .items}}{{range .spec.containers}}{{printf &amp;#34;%s\n&amp;#34; .image}}{{end}}{{end}}&amp;#39; 16istio/examples-bookinfo-details-v1:1.5.0 17istio/examples-bookinfo-productpage-v1:1.5.0 18istio/examples-bookinfo-ratings-v1:1.5.0 19 20# 条件判断 21# 列出所有不可调度节点的节点名与 IP 22kubectl get no -o go-template=&amp;#39;{{range .items}}{{if .spec.unschedulable}}{{.metadata.name}} {{.spec.externalID}}{{&amp;#34;\n&amp;#34;}}{{end}}{{end}}&amp;#39; 23 24 25# jsonpath 26 27# 查询pod的启动时间 28kubectl -n kube-system get pods -o=jsonpath=&amp;#39;{range .</description>
    </item>
    
    <item>
      <title>Etcd 使用入门</title>
      <link>https://example.com/posts/get-start-etcd/</link>
      <pubDate>Sat, 15 Jun 2019 12:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/get-start-etcd/</guid>
      <description>etcd是coreos根据Raft一致性算法用go实现的分布式KV数据库，作为k8s数据唯一存储的地方，etcd具有高性能的读写以及watch性能，也可以用在服务注册，配置中心等。官方Doc https://github.com/etcd-io/etcd/blob/master/Documentation
入门 初识单节点 etcd使用了两个端口 2379 grpc 以及2380 http，现在最新的API版本是v3，之前的v2性能有一定问题，建议直接用v3的API。配置etcd可以通过环境变量或者命令行参数。Etcd会有一些默认参数，但是bind的地址都是127.0.0.1
1docker run -d --name etcd k8s.gcr.io/etcd:3.3.10 etcd 2# etcdmain 32019-06-25 06:26:30.323960 I | etcdmain: etcd Version: 3.3.10 42019-06-25 06:26:30.324127 I | etcdmain: Git SHA: 27fc7e2 52019-06-25 06:26:30.324138 I | etcdmain: Go Version: go1.10.4 62019-06-25 06:26:30.324153 I | etcdmain: Go OS/Arch: linux/amd64 72019-06-25 06:26:30.324162 I | etcdmain: setting maximum number of CPUs to 2, total number of available CPUs is 2 82019-06-25 06:26:30.</description>
    </item>
    
    <item>
      <title>k8s APIServer 动态准入控制器</title>
      <link>https://example.com/posts/k8s-apiserver-webhook/</link>
      <pubDate>Sat, 25 May 2019 22:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/k8s-apiserver-webhook/</guid>
      <description>当你在k8s集群中一个不存在的namespace中创建一个pod，会返回一个namespace not found类似的错误，你有想过这个是哪个组件完成的吗？对就是admission plugin。
从控制器说起 什么是准入插件，在APIServer中有这样一个参数--enable-admission-plugins，用于对创建的对象的合理性进行验证和二次修改。可选的参数如下所示：
 AlwaysAdmit, # 一直允许 AlwaysDeny, # 一直禁止 AlwaysPullImages, # 在启动容器之前总是去下载镜像，相当于每当容器启动前做一次用于是否有权使用该容器镜像的检查 DefaultStorageClass, # 默认的sc DefaultTolerationSeconds, DenyEscalatingExec, # 拒绝exec和attach命令到有升级特权的Pod的终端用户访问。如果集中包含升级特权的容器，而要限制终端用户在这些容器中执行命令的能力，推荐使用此插件 DenyExecOnPrivileged, EventRateLimit, # event 限流 ExtendedResourceToleration, ImagePolicyWebhook, LimitPodHardAntiAffinityTopology, LimitRanger, # 默认资源限额，用于Pod和容器上的配额管理，它会观察进入的请求，确保Pod和容器上的配额不会超标。准入控制器LimitRanger和资源对象LimitRange一起实现资源限制管理 MutatingAdmissionWebhook, # 授权后的webhook NamespaceAutoProvision, NamespaceExists,# 命名空间是否存在 NamespaceLifecycle, # 当一个请求是在一个不存在的namespace下创建资源对象时，该请求会被拒绝。当删除一个namespace时，将会删除该namespace下的所有资源对象 NodeRestriction, OwnerReferencesPermissionEnforcement, PersistentVolumeClaimResize, # PVC 扩容 PersistentVolumeLabel, # PVC标签 PodNodeSelector,# pod 节点选择器 PodPreset, # 向namespace注入变量和存储 PodSecurityPolicy, # pod安全策略 PodTolerationRestriction, Priority, # 调度策略？ ResourceQuota, # 用于namespace上的配额管理，它会观察进入的请求，确保在namespace上的配额不超标。推荐将这个插件放到准入控制器列表的最后一个。ResourceQuota准入控制器既可以限制某个namespace中创建资源的数量，又可以限制某个namespace中被Pod请求的资源总量。ResourceQuota准入控制器和ResourceQuota资源对象一起可以实现资源配额管理 SecurityContextDeny, # 将Pod定义中定义了的SecurityContext选项全部失效。SecurityContext包含在容器中定义了操作系统级别的安全选型如fsGroup，selinux等选项 ServiceAccount, #这个插件实现了serviceAccounts等等自动化，如果使用ServiceAccount对象，强烈推荐使用这个插件 StorageObjectInUseProtection, TaintNodesByCondition, ValidatingAdmissionWebhook.</description>
    </item>
    
    <item>
      <title>使用kubeadm初始化集群</title>
      <link>https://example.com/posts/create-cluster-use-kubeadm/</link>
      <pubDate>Fri, 17 May 2019 22:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/create-cluster-use-kubeadm/</guid>
      <description>很久之前就写过这篇笔记，一直没发出来，虽说这样的文章差不多已经烂大街了，还是可以看一下的。
1⎈⎈ 2apt install -y bash-completion 3yum install -y bash-completion 4echo &amp;#34;source &amp;lt;(kubectl completion bash)&amp;#34; &amp;gt;&amp;gt; ~/.bashrc 5echo &amp;#34;source &amp;lt;(kubeadm completion bash)&amp;#34; &amp;gt;&amp;gt; ~/.bashrc 6source ~/.bashrc 7modprobe ip_vs &amp;amp;&amp;amp; modprobe ip_vs_rr &amp;amp;&amp;amp; modprobe ip_vs_wrr &amp;amp;&amp;amp; modprobe ip_vs_sh 8rm -rf /var/lib/cni/ 9rm -rf /var/lib/kubelet/* 10rm -rf /etc/cni/ 11ip link del cni0 &amp;amp;&amp;amp; ip link del flannel.1 &amp;amp;&amp;amp; ip link del kube-ipvs0 12net.ipv6.conf.all.disable_ipv6 = 1 13awk &amp;#39;$2 ~ path {print $2}&amp;#39; path=/var/lib/kubelet /proc/mounts | xargs -r umount 14kubeadm init --kubernetes-version 1.</description>
    </item>
    
    <item>
      <title>Kubernetes中的存储</title>
      <link>https://example.com/posts/storage-in-kubernets/</link>
      <pubDate>Sun, 12 May 2019 22:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/storage-in-kubernets/</guid>
      <description>概念 PersistentVolume PersistentVolume（持久卷，简称PV）是集群内，由管理员提供的网络存储的一部分。就像集群中的节点一样，PV也是集群中的一种资源。它也像Volume一样，是一种volume插件，但是它的生命周期却是和使用它的Pod相互独立的。PV这个API对象，捕获了诸如NFS、ISCSI、或其他云存储系统的实现细节。 https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistent-volumes
1# kubectl explain pv.spec 2# KIND: PersistentVolume 3# VERSION: v1 4# spec 5# accessModes &amp;lt;[]string&amp;gt; 6# capacity &amp;lt;map[string]string&amp;gt; 7# mountOptions &amp;lt;[]string&amp;gt; 部分支持，比如NFS 8# persistentVolumeReclaimPolicy &amp;lt;string&amp;gt; 9# storageClassName &amp;lt;string&amp;gt; 10# volumeMode &amp;lt;string&amp;gt; 11apiVersion: v1 12kind: PersistentVolume 13metadata: 14 name: block-pv 15spec: 16 accessModes: 17 - ReadWriteOnce 18 capacity: 19 storage: 10Gi 20 persistentVolumeReclaimPolicy: Retain 21 storageClassName： aliclood-nas 22 volumeMode: Block PersistentVolumeClaim PersistentVolumeClaim（持久卷声明，简称PVC）是用户的一种存储请求。它和Pod类似，Pod消耗Node资源，而PVC消耗PV资源。Pod能够请求特定的资源（如CPU和内存）。PVC能够请求指定的大小和访问的模式（可以被映射为一次读写或者多次只读）。 PVC允许用户消耗抽象的存储资源，用户也经常需要各种属性（如性能）的PV。集群管理员需要提供各种各样、不同大小、不同访问模式的PV，而不用向用户暴露这些volume如何实现的细节。因为这种需求，就催生出一种StorageClass资源。
1 # accessModes &amp;lt;[]string&amp;gt; 2 # storageClassName &amp;lt;string&amp;gt; 3 # volumeMode &amp;lt;string&amp;gt; 4 # volumeName &amp;lt;string&amp;gt; 5 # resources &amp;lt;Object&amp;gt; 6kind: PersistentVolumeClaim 7apiVersion: v1 8metadata: 9 name: myclaim 10spec: 11 accessModes: 12 - ReadWriteOnce 13 storageClassName: slow 14 volumeMode: Filesystem 15 resources: 16 requests: 17 storage: 8Gi StorageClass StorageClass提供了一种方式，使得管理员能够描述他提供的存储的等级。集群管理员可以将不同的等级映射到不同的服务等级、不同的后端策略。 https://kubernetes.</description>
    </item>
    
    <item>
      <title>在aws上使用kops部署k8s集群</title>
      <link>https://example.com/posts/use-kops-in-aws/</link>
      <pubDate>Sun, 12 May 2019 22:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/use-kops-in-aws/</guid>
      <description>简介：Kubernetes Operations (kops) - Production Grade K8s Installation, Upgrades, and Management kops宣称已经达到生产级别，准备试一下kops，观察其运行控制平面的方式。官方的文档如下： https://github.com/kubernetes/kops/blob/master/docs/aws.md
安装 当然kops也是用golang写的，所以你只要下载预编译的二进制文件即可。
1# 指定版本 2https://github.com/kubernetes/kops/releases/download/1.10.0/kops-linux-amd64 3 4#最新版本 5wget -O kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d &amp;#39;&amp;#34;&amp;#39; -f 4)/kops-linux-amd64 6chmod +x ./kops 7sudo mv ./kops /usr/local/bin/ 8 9curl -LO https://storage.googleapis.com/kubernetes-release/release/1.10.0/bin/linux/amd64/kubectl 10 11wget -O kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl 12chmod +x ./kubectl 13sudo mv ./kubectl /usr/local/bin/kubectl 14 15#awscli是用Python写的，安装Python和pip后直接运行下面命令就可以了。 16pip install awscli 创建IAM kops 需要有以下四个权限，首先创建了kops的用户组，然后创建了kops用户
 AmazonEC2FullAccess AmazonRoute53FullAccess AmazonS3FullAccess IAMFullAccess AmazonVPCFullAccess  1aws iam create-group --group-name kops 2 3aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops 4aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops 5aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops 6aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops 7aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops 8 9aws iam create-user --user-name kops 10 11aws iam add-user-to-group --user-name kops --group-name kops 12 13aws iam create-access-key --user-name kops 14 15# configure the aws client to use your new IAM user 16aws configure # Use your new access and secret key here 17aws iam list-users # you should see a list of all your IAM users here 18 19# Because &amp;#34;aws configure&amp;#34; doesn&amp;#39;t export these vars for kops to use, we export them now 20export AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id) 21export AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) 创建S3 首先我创建了一个s3的存储，用于存放kops的配置文件，然后为这个存储桶加入了版本控制</description>
    </item>
    
    <item>
      <title>CoreDNS-基于Caddy的DNS Server</title>
      <link>https://example.com/posts/coredns/</link>
      <pubDate>Thu, 25 Apr 2019 22:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/coredns/</guid>
      <description>CoreDNS利用作为Web服务器Caddy的一部分而开发的服务器框架。该框架具有非常灵活，可扩展的模型，用于通过各种中间件组件传递请求。这些中间件组件根据请求提供不同的操作，例如记录，重定向，修改或维护。虽然它一开始作为Web服务器，但是Caddy并不是专门针对HTTP协议的，所以也是开发CoreDNS的理想框架。
Install 如果使用Kubeadm初始化的集群，默认已经安装了CoreDNS。默认配置文件如下：
1cat /etc/coredns/Corefile 2.:53 { 3 errors 4 health 5 kubernetes cluster.local in-addr.arpa ip6.arpa { 6 pods insecure 7 upstream 8 fallthrough in-addr.arpa ip6.arpa 9 } 10 prometheus :9153 11 proxy . /etc/resolv.conf 12 cache 30 13 reload 14 loadbalance 15} 9153/metrics 是默认的metrics端口 :8080/health 是健康检查的端口，除此之外需要注意的是添加了一个kernel capabilities，DNS策略是默认。只会对cluster.local进行解析，其他域名则是转发到上游DNS服务器。
 securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all CoreDNS相当于创建了一个Kubernetes中间件。该中间件使用Kubernetes API来满足针对特定Kubernetes pod或服务的DNS请求。而且由于Kube-DNS作为Kubernetes的另一项服务，kubelet和CoreDNS之间没有紧密的绑定。只需要将DNS服务的IP地址和域名传递给kubelet，而Kubernetes并不关心谁在实际处理该IP请求。和DNS相关的配都在kubelet上面，分别是：
 &amp;ndash;cluster-dns &amp;ndash;cluster-domain  除此之外Pod的dnsPolicy 可选参数为&amp;rsquo;ClusterFirstWithHostNet&#39;, &amp;lsquo;ClusterFirst&amp;rsquo;, &amp;lsquo;Default&amp;rsquo; or &amp;lsquo;None&amp;rsquo;，而当Pod设置dnsPolicy为ClusterFirst时 ，即可在pod内生成以下/etc/resolv.</description>
    </item>
    
    <item>
      <title>k8s 的实时日志 Events</title>
      <link>https://example.com/posts/k8s-event-explorer/</link>
      <pubDate>Tue, 16 Apr 2019 09:43:24 +0000</pubDate>
      
      <guid>https://example.com/posts/k8s-event-explorer/</guid>
      <description>Event 什么是k8s的Events？比如你启动了一个Deployment，k8s的各大组件就开始依次忙碌起来，最终完成Pod的创建，从声明一个Deployment开始，到Pod启动完成，会生成一些列Events，用来告知用户现在的状态。event就是用来回答一些问题，比如为什么pod没有启动，因为没有配置私有仓库的认证，为什么pod会被Kill，因为是超过limit的限制。具体如下所示：
1kubectl get ev 2LAST SEEN TYPE REASON OBJECT MESSAGE 33s Normal Scheduled pod/nginx-698898f666-smg7t Successfully assigned default/nginx-698898f666-smg7t to east1-monitor1 43s Normal Pulled pod/nginx-698898f666-smg7t Container image &amp;#34;nginx:alpine&amp;#34; already present on machine 53s Normal Created pod/nginx-698898f666-smg7t Created container nginx 63s Normal Started pod/nginx-698898f666-smg7t Started container nginx 73s Normal SuccessfulCreate replicaset/nginx-698898f666 Created pod: nginx-698898f666-smg7t 83s Normal ScalingReplicaSet deployment/nginx Scaled up replica set nginx-698898f666 to 1 这些信息会被存储在Etcd中，默认的保存时间为1小时。
1/registry/events/default/nginx-698898f666-smg7t.1593fd25ca0c7e00 2/registry/events/default/nginx-698898f666-smg7t.1593fd25f1ff3cca 3/registry/events/default/nginx-698898f666-smg7t.1593fd25f4066f9f 4/registry/events/default/nginx-698898f666-smg7t.1593fd25fd741cf6 5/registry/events/default/nginx-698898f666.1593fd25c9abf7dc 6/registry/events/default/nginx.1593fd25c9217fce 找到一条Event，可以看到完整的信息如下，值得关注的地方有involvedObject的name和source的host等等。</description>
    </item>
    
    <item>
      <title>k8s 七层访问入口 Ingress</title>
      <link>https://example.com/posts/k8s-ingress-useage/</link>
      <pubDate>Tue, 12 Mar 2019 22:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/k8s-ingress-useage/</guid>
      <description>k8s初期的时候，只有Service作为四层负载均衡，后来才推出了专门用作七层的负载均衡&amp;ndash;Ingress。第一批实现Ingress Controller的就是traefik以及Nginx Ingress，后来随着Envory的出现，也逐渐涌现出了越来越多的Ingress Controller实现，比如：
 https://github.com/heptio/contour https://github.com/datawire/ambassador https://github.com/istio/istio/  但是这些实现的原理说到底就是watch后端的Service，然后创建对于的访问规则。
Install 以Nginx Ingress为例，由Go，Lua和C三种语言组成，Go负责与k8s API交互，下面简单介绍一些安装过程。
1#首先安装控制器， 2kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml 3#然后创建Service 4kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/baremetal/service-nodeport.yaml 以上是属于Bate-metal的配置，如果你是在Public CLoud可以替换为LoadBalancer。仔细观察一下第一个yaml文件，可以发现有些特殊的地方，如下所示，有一个特殊的annotations，看来和APIServer类似，支持多副本高可用。
I0411 09:11:06.783582 7 leaderelection.go:227] successfully acquired lease ingress-nginx/ingress-controller-leader-nginx
1kubectl -n ingress-nginx get cm ingress-controller-leader-nginx -oyaml 2apiVersion: v1 3kind: ConfigMap 4metadata: 5 annotations: 6 control-plane.alpha.kubernetes.io/leader: &amp;#39;{&amp;#34;holderIdentity&amp;#34;:&amp;#34;nginx-ingress-controller-85f9f75759-4wb8k&amp;#34;,&amp;#34;leaseDurationSeconds&amp;#34;:30,&amp;#34;acquireTime&amp;#34;:&amp;#34;2019-04-11T09:11:06Z&amp;#34;,&amp;#34;renewTime&amp;#34;:&amp;#34;2019-04-11T13:12:56Z&amp;#34;,&amp;#34;leaderTransitions&amp;#34;:0}&amp;#39; 7 creationTimestamp: &amp;#34;2019-04-11T09:11:06Z&amp;#34; 8 name: ingress-controller-leader-nginx 9 namespace: ingress-nginx 10 resourceVersion: &amp;#34;28998&amp;#34; 11 selfLink: /api/v1/namespaces/ingress-nginx/configmaps/ingress-controller-leader-nginx 12 uid: bd25d320-5c39-11e9-af65-00163e132347 因为Nginx原生支持四层负载均衡，所有Nginx Ingress也是支持四层的。</description>
    </item>
    
    <item>
      <title>Kubernetes 中的网络结构</title>
      <link>https://example.com/posts/kubernetes-network-flannel/</link>
      <pubDate>Tue, 12 Feb 2019 22:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/kubernetes-network-flannel/</guid>
      <description>Kubernetes 采用的是基于扁平地址空间的网络模型，集群中的每个 Pod 都有自己的 IP 地址，Pod 之间不需要配置 NAT 就能直接通信。另外，同一个 Pod 中的容器共享 Pod 的 IP，能够通过 localhost 通信。
  IP-per-Pod，每个 Pod 都拥有一个独立 IP 地址，Pod 内所有容器共享一个网络命名空间
  集群内所有 Pod 都在一个直接连通的扁平网络中，可通过 IP 直接访问
 所有容器之间无需 NAT 就可以直接互相访问 所有 Node 和所有容器之间无需 NAT 就可以直接互相访问 容器自己看到的 IP 跟其他容器看到的一样    接下来我们来先介绍一下几个概念，
 Service cluster IP 尽可在集群内部访问，外部请求需要通过 NodePort、LoadBalance 或者 Ingress 来访问 网络的命名空间：Linux在网络栈中引入网络命名空间，将独立的网络协议栈隔离到不同的命令空间中，彼此间无法通信；docker利用这一特性，实现不容器间的网络隔离。 Veth设备对：Veth设备对的引入是为了实现在不同网络命名空间的通信。 Iptables/Netfilter：Netfilter负责在内核中执行各种挂接的规则(过滤、修改、丢弃等)，运行在内核 模式中；Iptables模式是在用户模式下运行的进程，负责协助维护内核中Netfilter的各种规则表；通过二者的配合来实现整个Linux网络协议栈中灵活的数据包处理机制。 网桥：网桥是一个二层网络设备,通过网桥可以将linux支持的不同的端口连接起来,并实现类似交换机那样的多对多的通信。 路由：Linux系统包含一个完整的路由功能，当IP层在处理数据发送或转发的时候，会使用路由表来决定发往哪里  如今，Kubernetes改变了软件开发的完成方式。作为用于管理容器化工作负载和服务的便携式，可扩展的开源平台，该平台可促进声明式配置和自动化，Kubernetes已证明自己是管理复杂微服务的主导者。它的受欢迎程度源于Kubernetes满足以下需求的事实：企业希望增长和减少支付，DevOps需要一个稳定的平台，可以大规模运行应用程序，开发人员想要可靠且可复制的流程来编写，测试和调试代码。这是一篇很好的文章，可以了解有关Kubernetes演化和架构的更多信息。
管理Kubernetes网络的重要领域之一是在内部和外部转发容器端口，以确保容器和Pod可以正确相互通信。为了管理此类通信，Kubernetes提供以下四种联网模型：
 容器到容器通信 点对点通讯 Pod到Service通讯 对外沟通  在本文中，我们通过向您展示Kubernetes网络中的Pod可以相互通信的方式，深入探讨Pod到Pod的通信。</description>
    </item>
    
    <item>
      <title>k8s 集群监控 --  Prometheus Operator</title>
      <link>https://example.com/posts/promthus-operator/</link>
      <pubDate>Tue, 22 Jan 2019 22:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/promthus-operator/</guid>
      <description>对于任何系统来说，监控都是不可或缺的东西，系统现在是什么样，与一周前相比是否需要调整资源等都需要一个监控系统来参考。对于k8s来说常用的开源监控方案也就是下面几种：
 heaspter+InfluxDB (已被弃用) Prometheus Open falcon Telegraf+InfluxDB  而且k8s所需的监控项目是比较多的，如下表：(k8s 1.14.0)    监控组件 metric PATＨ     API Server 6443/metrics   Controller Ｍanager 10257/metrics   kube Scheduler 10259/metrics   Etcd 2379/metrics   Kubelet API 10252/metrics   Cadvisor 10250/metrics/cadvisor   Node 9100/metrics    因此想要全面的监控集群状态，我个人感觉Prometheus是比较合适的，但是当你修改依次规则就需要重启一次。而且云原生中比较推荐使用声明式代替命令式，于是Prometheus Operator就诞生了，整体结构如下图：
Install 个人推荐的方式是从repo中apply一堆yaml，如果你擅长helm，也是可以的。
1git clone https://github.com/coreos/prometheus-operator 2kubectl get crd 3NAME CREATED AT 4alertmanagers.monitoring.coreos.com 2018-09-12T02:13:19Z 5prometheuses.monitoring.coreos.com 2018-09-12T02:13:19Z 6prometheusrules.</description>
    </item>
    
    <item>
      <title>一个完整的Pod 描述应该是什么样的？</title>
      <link>https://example.com/posts/k8s-pod-full-yaml/</link>
      <pubDate>Tue, 25 Dec 2018 22:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/k8s-pod-full-yaml/</guid>
      <description>Pod作为k8s工作负载的基本单位,不论是Deployment还是job等等需要计算资源的控制器，都需要定义将被创建的pod模板，所以熟悉pod的所有字段还是很有必要的。今天就用Deployment作为例子，看一下一个完整pod模板应该是什么样子的。
Deployment大致可以分为五个字段，可以用kubectl explain deploy命令查看。前两个字段分别为apiverion和kind，就不用多说了，后三个字段是metadata，spec和k8s生成的status，容我慢慢讲来。
三大描述 metadata 此部分主要定义了pod的属性信息，注解和标签，名字，归属信息，是被哪个控制器创建的，pod-template-hash是必定存在的标签，仔细看会发现和rs的名字后面是相同的。 示例：
metadata: creationTimestamp: &amp;quot;2019-05-18T05:13:16Z&amp;quot; generateName: nginx-6c885545f8- labels: pod-template-hash: 6c885545f8 run: nginx name: nginx-6c885545f8-gl8gc namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: nginx-6c885545f8 uid: a4dfdd64-792b-11e9-8da2-00163e132347 resourceVersion: &amp;quot;1178868&amp;quot; selfLink: /api/v1/namespaces/default/pods/nginx-6c885545f8-gl8gc uid: a4e0e290-792b-11e9-8da2-00163e132347 spec 终于到了pod的描述部分，最主要的containers部分就是在这里定义的，也是一个object，我们知道pod内的containers存储和网络等资源是共享的，CPU和memory也是可以隔离的，默认挂载了一个volume，也就是serviceaccount用来在内部访问APIServer所用的token，CA信息。 按种类大致分为以下几种
  调度相关
 affinity 亲和性 分为nodeAffinity podAffinity podAntiAffinity三种 nodeName 节点名字 默认由调度器分配 nodeSelector 节点选择器 schedulerName 调度器名字 tolerations 容忍    生命周期
 initContainers 初始化容器 containers 的 lifecycle 包括preStart和preStop脚本 restartPolicy 重启策略 Always, OnFailure,Never.</description>
    </item>
    
    <item>
      <title>Kubernetes 中的资源管理和优先级</title>
      <link>https://example.com/posts/k8s-resource-qos/</link>
      <pubDate>Tue, 09 Oct 2018 22:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/k8s-resource-qos/</guid>
      <description>Kubernetes 是怎么定义资源的呢？
​	在 kubernetes 中，任何可以被申请、分配，最终被使用的对象，都是 kubernetes 中的资源，比如 CPU、内存。而且针对每一种被 kubernetes 所管理的资源，都会被赋予一个【资源类型名】，这些名字都是符合 RFC 1123 规则的，比如 CPU，它对应的资源名全称为 kubernetes.io/cpu（在展示时一般直接简写为 cpu）；GPU 资源对应的资源名为 alpha.kubernetes.io/nvidia-gpu。
​	除了名字以外，针对每一种资源还会对应有且仅有一种【基本单位】。这个基本单位一般是在 kubernetes 组件内部统一用来表示这种资源的数量的，比如 memory 的基本单位就是字节。但是为了方便开发者使用，在开发者通过 yaml 文件或者 kubectl 同 kubernetes 进行交互时仍可以使用多种可读性更好的资源单位，比如内存的 Gi。而当这些信息进入 kubernetes 内部后，还是会被显式的转换为最基本单位。
​	所有的资源类型，又可以被划分为两大类：可压缩(compressible)和不可压缩(incompressible)的。其评判标准就在于：如果系统限制或者缩小容器对可压缩资源的使用的话，只会影响服务对外的服务性能，比如 CPU 就是一种非常典型的可压缩资源。对于不可压缩资源来说，资源的紧缺是有可能导致服务对外不可用的，比如内存就是一种非常典型的不可压缩资源。
Kubernetes 中有哪几类资源呢？
​	目前 kubernetes 默认带有两类基本资源CPU和Memory。其中 CPU，不管底层的机器是通过何种方式提供的（物理机 or 虚拟机），一个单位的 CPU 资源都会被标准化为一个标准的 &amp;ldquo;Kubernetes Compute Unit&amp;rdquo; ，大致和 x86 处理器的一个单个超线程核心是相同的。CPU 资源的基本单位是 millicores，因为 CPU 资源其实准确来讲，指的是 CPU 时间。所以它的基本单位为 millicores，1 个核等于 1000 millicores。也代表了 kubernetes 可以将单位 CPU 时间细分为 1000 份，分配给某个容器。内存memory 资源的基本单位比较好理解，就是字节。另外，kubernetes 针对用户的自定制需求，还为用户提供了 device plugin 机制，让用户可以将资源类型进一步扩充，比如现在比较常用的 nvidia gpu 资源。这个特性可以使用户能够在 kubernetes 中管理自己业务中特有的资源，并且无需修改 kubernetes 自身源码。而且 kubernetes 自身后续还会进一步支持更多非常通用的资源类型，比如网络带宽、存储空间、存储 iops 等等。所以如上，我们就回答了 kubernetes 是如何定义资源的问题。</description>
    </item>
    
    <item>
      <title>Kubernetes 中的弹性伸缩 Autoscaler</title>
      <link>https://example.com/posts/cluster-autoscaler/</link>
      <pubDate>Tue, 25 Sep 2018 12:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/cluster-autoscaler/</guid>
      <description>弹性伸缩是一个比较吸引人的话题，类似与云计算中所提倡的按需付费，合理使用可以减少所需计算资源，降低相关的服务器成本。对于k8s来说目前一共有两种弹性伸缩：
 Cluster 即自动增加删除节点 Pod 分为水平和垂直两种，水平伸缩在k8s里已经支持，垂直伸缩需要安装  相关repo https://github.com/kubernetes/autoscaler ，下面分别介绍下相关信息。
Node Cluster-autoscaler ClusterScaler安装相对来说比较简单，和其他插件类似，定义好ServiceAccount，Deployment以及伸缩组等即可，但是比较强依靠公有云。目前支持的AliCloud，Azure，AWS，BaiduCloud等，注意需要配置Access Key以及根证书（访问API时需要）等信息。关键配置如下：
1 - ./cluster-autoscaler 2 - --v=4 3 - --stderrthreshold=info 4 - --cloud-provider=aws 5 - --skip-nodes-with-local-storage=false 6 - --scale-down-delay-after-add=3m 7 - --nodes=1:5:nodes.demo.k8s.local 按照正常步骤，向集群添加节点一共需要三步：
 新开一台机器， 配置bootstap token等信息， 最后加入集群。  ClusterScaler相当于替我们做了这些工作，仔细阅读相关文档，可以看到阿里云依赖的是ESS，而kops下的aws依赖的是ASG（auto scaling group），通过自动伸缩组可以几乎无干预的完成扩容。
这里穿插一个小知识，对于大多数公有云来说，都支持一个叫做cloud-init的东西，也叫做user data，也就是在机器初始化的时候执行的一段shell脚本，这也就是上面的第二步。
那么它是如何判断集群需要扩容、缩容呢？我们先启动一个request为4C8G的nginx做测试，或者是5个1C2G的nginx，目的就是超过现有集群剩余的计算资源，正常情况下一定会有pod处于pending状态，这时候通过观察日志可以发现，ClusterScaler就会触发扩容节点。
I0408 04:46:52.022024 1 scale_up.go:249] Pod default/demo-589b486fbb-r5fn4 is unschedulable I0408 04:46:52.022039 1 scale_up.go:249] Pod default/demo-589b486fbb-rzlpn is unschedulable 源码在https://github.com/kubernetes/autoscaler/blob/cluster-autoscaler-1.2.2/cluster-autoscaler/core/scale_up.go#L59
Pod HPA 水平伸缩 现在官方支持的就是HorizontalPodAutoscaler，可以根据CPU和memory使用情况来判断伸缩，需要注意的是，伸缩的条件是CPU利用率超过多少就触发扩容，所以至少需要两个参数，pod现在的CPU，以及它request的CPU， 所以我们首先需要部署metric-server，用来获取pod的实时CPU使用情况。 https://github.</description>
    </item>
    
    <item>
      <title>terraform provider Kubernetes 入门</title>
      <link>https://example.com/posts/terraform/</link>
      <pubDate>Mon, 17 Sep 2018 11:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/terraform/</guid>
      <description>下载
初始化 terraform 扩展主要靠的就是provider，所以在使用之前需要准备好或者下载对应的provider。至于你需要什么provider，需要在main.tf指定，如下所示。
如果可以联网的话，貌似会从fastly的CDN上下载，默认放在~/.terraform/plugins目录，离线安装或者自己build的话也可以放在对应的目录即可。
terraform默认会读取kubeconfig的默认配置，有点绕口，也可以自己配置对应的证书或者basic auth，这一点可以去看官方文档，我配置的就是读取默认配置。然后就就可以初始化去下载k8s的provider了。
1cat main.tf 2provider &amp;#34;kubernetes&amp;#34; {} 3# 初始化 4terraform init 5 6Initializing the backend... 7 8Initializing provider plugins... 9- Checking for available provider plugins... 10- Downloading plugin for provider &amp;#34;kubernetes&amp;#34; (hashicorp/kubernetes) 1.9.0... 11 12The following providers do not have any version constraints in configuration, 13so the latest version was installed. 14 15To prevent automatic upgrades to new major versions that may contain breaking 16changes, it is recommended to add version = &amp;#34;.</description>
    </item>
    
    <item>
      <title>Kubernetes 认证与授权</title>
      <link>https://example.com/posts/kubernetes-auth-rbac/</link>
      <pubDate>Fri, 15 Jun 2018 22:31:20 +0800</pubDate>
      
      <guid>https://example.com/posts/kubernetes-auth-rbac/</guid>
      <description>对于一个系统来说，认证和授权不可或缺的部分，最常见的方式要数账号密码，管理员和普通用户这种认证方式，也就是常说的ABAC。但是每次修改权限都需要重启，或者修改记录，会有些不方便，于是基于角色的访问控制&amp;ndash;RBAC就出现了，同时k8s分离了认证，授权和准入的过程，方便第三方开发相关插件。
下图大致描述了k8s认证的流程图，描述了一个请求在经过APIServer的时候都经历了哪些鉴权的过程。如下图所示：需要访问API的有人类，也就是Ops，通过kubectl以及Kubeconfig配置文件和API-Server交互。还有就是Pod，也就是应用，对API资源进行CURD。然后依次经过了身份认证(authentication)、授权(authorization)和准入控制（admission control）。下面就来详细介绍下认证和授权
认证  Authentication：即身份验证，这个环节它面对的输入是整个http request，它负责对来自client的请求进行身份校验，支持的方法包括：  client证书验证（https双向验证） basic auth 普通token jwt token(用于serviceaccount)    APIServer启动时，可以指定一种Authentication方法，也可以指定多种方法。如果指定了多种方法，那么APIServer将会逐个使用这些方法对客户端请求进行验证，只要请求数据通过其中一种方法的验证，APIServer就会认为Authentication成功；在较新版本kubeadm引导启动的k8s集群的apiserver初始配置中，默认支持client证书验证和serviceaccount两种身份验证方式。在这个环节，apiserver会通过client证书或http header中的字段(比如serviceaccount的jwt token)来识别出请求的“用户身份”，包括”user”、”group”等，这些信息将在后面的authorization环节用到。
Client 证书 首先我们看一下kubectl的默认配置文件~/.kube/config
1apiVersion: v1 2clusters: 3- cluster: 4 certificate-authority-data: REDACTED 5 server: https://172.16.66.101:6443 6 name: kubernetes 7contexts: 8- context: 9 cluster: kubernetes 10 user: kubernetes-admin 11 name: kubernetes-admin@kubernetes 12current-context: kubernetes-admin@kubernetes 13kind: Config 14preferences: {} 15users: 16- name: kubernetes-admin 17 user: 18 client-certificate-data: REDACTED 19 client-key-data: REDACTED 这有两个重要的信息 client-certificate-data, client-key-data,首先把data提取出来</description>
    </item>
    
  </channel>
</rss>
