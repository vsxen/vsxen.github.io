[{"content":"","permalink":"https://vsxen.github.io/posts/2025-08-10/","summary":"","title":"2025 08 10"},{"content":"这一周好忙\n新公司的网络居然可以直连谷歌，香港的出口IP，Zenlayer的线路哈哈，而且还能直接用Gemini神奇。\n发现git默认生成的密钥对改成了ed25519，问了下AI如下：\nssh-keygen 是一个非常实用的命令行工具，用于生成 SSH (Secure Shell) 协议使用的密钥对。这个密钥对通常由一个私钥和一个公钥组成。它们是 SSH 安全通信的基石，允许用户在不传输密码的情况下安全地认证到远程服务器。\n密钥对的核心概念 公钥 (Public Key)： 这是您可以安全地共享给任何人的部分。 您需要将它放在您想要连接的远程服务器上（通常在 ~/.ssh/authorized_keys 文件中）。 公钥用于加密数据，并且可以验证私钥签名的数据。 它不能用来解密数据或进行身份认证，只能用于验证私钥持有者的身份。 私钥 (Private Key)： 这是您需要严格保密的部分。绝不能与任何人共享。 它通常存储在您的本地计算机上（例如 ~/.ssh/id_rsa）。 私钥用于解密由对应公钥加密的数据，并用于向远程服务器签名认证请求。 如果私钥泄露，您的 SSH 连接安全就会受到威胁。 ssh-keygen 的基本用法 最简单的生成命令是：\nssh-keygen 执行这个命令后，ssh-keygen 会引导您完成以下几个步骤：\n文件保存路径： 它会提示您选择密钥对的保存位置。默认情况下，私钥会保存在 ~/.ssh/id_rsa（对于 RSA 算法）或 ~/.ssh/id_ed25519（对于 Ed25519 算法），公钥则会是相同路径加上 .pub 后缀（例如 ~/.ssh/id_rsa.pub）。\nEnter file in which to save the key (/home/user/.ssh/id_rsa): 如果您不输入任何内容直接回车，就会使用默认路径。如果您想生成多个密钥对用于不同目的，可以输入不同的文件名。\n密码短语 (Passphrase)：\nEnter passphrase (empty for no passphrase): Enter same passphrase again: 这是非常重要的一步。\n作用： 为您的私钥加密。即使私钥文件被盗，没有密码短语也无法使用。这提供了额外的安全层。 建议： 强烈建议设置一个强密码短语。虽然每次使用私钥时都需要输入密码短语会增加一点点不便，但它能极大地提升安全性。您可以使用 ssh-agent 来缓存密码短语，避免每次都输入。 留空： 如果您留空（不设置密码短语），私钥将不加密，这意味着任何获得该私钥文件的人都可以直接使用它。在自动化脚本等特定场景下可能会留空，但请务必了解其安全风险。 密钥指纹和随机艺术图： 生成完成后，ssh-keygen 会显示密钥的指纹 (fingerprint) 和一个随机生成的艺术图 (randomart image)。这些信息用于视觉验证密钥的唯一性。\nYour identification has been saved in /home/user/.ssh/id_rsa Your public key has been saved in /home/user/.ssh/id_rsa.pub The key fingerprint is: SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx user@hostname The key\u0026#39;s randomart image is: +---[RSA 3072]----+ | . . | | . + | | o + | | E o + | | . o + o . | | . + = + o | | . + O . | | . = . | | + | +----[SHA256]-----+ ssh-keygen 的常用选项 ssh-keygen 提供了许多选项来定制密钥生成过程：\n-t \u0026lt;type\u0026gt;：指定密钥类型。 rsa (默认)：最常用的类型，通常足够安全。 dsa：较旧，不推荐使用。 ecdsa：椭圆曲线数字签名算法，通常提供与 RSA 相同安全级别但密钥更短。 ed25519：目前推荐的算法，基于 Edwards 曲线，速度快且安全性高，抗量子攻击特性更好。 示例： ssh-keygen -t ed25519 -b \u0026lt;bits\u0026gt;：指定密钥的位数 (bits)。 适用于 RSA、DSA、ECDSA。对于 Ed25519，位数是固定的，不需要指定。 RSA 建议至少 2048 位，更安全的选择是 3072 或 4096 位。 示例： ssh-keygen -t rsa -b 4096 -f \u0026lt;filename\u0026gt;：指定密钥文件保存的路径和名称。 示例： ssh-keygen -f ~/.ssh/my_webserver_key -P \u0026lt;passphrase\u0026gt;：指定旧的密码短语。 (用于更改密码短语时) -N \u0026lt;new_passphrase\u0026gt;：指定新的密码短语。 (用于更改密码短语时) -C \u0026lt;comment\u0026gt;：为公钥添加注释。 这个注释不会影响密钥的功能，但有助于您识别密钥的用途。 默认注释是 user@hostname。 示例： ssh-keygen -t rsa -b 4096 -C \u0026quot;My key for production server\u0026quot; -q：静默模式。 减少输出信息。 -p：改变现有私钥的密码短语。 示例： ssh-keygen -p -f ~/.ssh/id_rsa -y：从私钥中提取公钥。 如果您不小心删除了 .pub 文件，可以用这个命令重新生成。 示例： ssh-keygen -y -f ~/.ssh/id_rsa \u0026gt; ~/.ssh/id_rsa.pub 生成的密钥文件内容 私钥文件 (id_rsa 或 id_ed25519)：\n-----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAAZXNjZHNhLXNoYTItbmlzdHAyNTYAAA... -----END OPENSSH PRIVATE KEY----- 这是一个 PEM 编码的文件，包含您的私钥信息。绝对不能泄露。\n公钥文件 (id_rsa.pub 或 id_ed25519.pub)：\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDFf... user@hostname 或者对于 Ed25519：\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICL... user@hostname 公钥文件通常只有一行。它包含三个部分：\n密钥类型： ssh-rsa 或 ssh-ed25519。 公钥本身： 一长串 Base64 编码的字符串。 注释： 默认是 user@hostname，或您用 -C 指定的注释。 密钥对的使用流程 生成密钥对： 在您的本地计算机上使用 ssh-keygen 生成私钥和公钥。 上传公钥： 将生成的公钥 (.pub 文件) 的内容复制到您想要连接的远程服务器的 ~/.ssh/authorized_keys 文件中。如果 ~/.ssh 目录或 authorized_keys 文件不存在，您需要手动创建它们，并确保它们有正确的权限（chmod 700 ~/.ssh 和 chmod 600 ~/.ssh/authorized_keys）。 安全连接： 从本地计算机使用 ssh user@remote_host 命令连接。SSH 客户端会自动查找并使用您的私钥进行身份认证。如果私钥有密码短语，SSH 客户端会提示您输入。 通过理解 ssh-keygen 及其生成的密钥对，您可以更安全、高效地管理您的 SSH 连接。\n排查了一天的网络不通的问题，最后重启一下好了，额有些东西搞不出来就先睡一觉吧。\n试了下 k3s 还是有点意思的，我以前以为只嵌入了控制面，才发现是全部都嵌入了，一个可执行文件就搞定了。\n试用了一下WSL2，最大的败笔就是不能同时启动多个发行版，多个发行版默认还是共享网络的，对于分布式开发就很不舒服。\n从 XML 到 JSON 再到 CBOR 尽管 Kubernetes API Server 使用 HTTPS 协议提供 RESTful API，但它确实使用了 Protocol Buffers (Protobuf) 来进行数据序列化。 同时 gRPC-Web 这可以说是将 Protobuf over HTTP 概念推广得最成功的项目之一。\n排查一个服务连不上etcd的问题，最后发现是没有配置证书：\n2025-07-30 01:47:11.007657 I | embed: rejected connection from \u0026#34;10.40.242.88:47286\u0026#34; (error \u0026#34;tls: failed to verify client\u0026#39;s certificate: x509: certificate signed by unknown authority (possibly because of \\\u0026#34;crypto/rsa: verification error\\\u0026#34; while trying to verify candidate authority certificate \\\u0026#34;demo\\\u0026#34;)\u0026#34;, ServerName \u0026#34;default-etcd-client.demo.svc\u0026#34;) helm 居然还会去检查资源有没有冲突：\nError: INSTALLATION FAILED: Unable to continue with install: StorageClass \u0026#34;local-path\u0026#34; in namespace \u0026#34;\u0026#34; exists and cannot be imported into the current release: invalid ownership metadata; label validation error: missing key \u0026#34;app.kubernetes.io/managed-by\u0026#34;: must be set to \u0026#34;Helm\u0026#34;; annotation validation error: missing key \u0026#34;meta.helm.sh/release-name\u0026#34;: must be set to \u0026#34;default\u0026#34;; annotation validation error: missing key \u0026#34;meta.helm.sh/release-namespace\u0026#34;: must be set to \u0026#34;demo\u0026#34; ","permalink":"https://vsxen.github.io/posts/2025-08-03/","summary":"\u003cp\u003e这一周好忙\u003c/p\u003e\n\u003cp\u003e新公司的网络居然可以直连谷歌，香港的出口IP，Zenlayer的线路哈哈，而且还能直接用Gemini神奇。\u003c/p\u003e\n\u003cp\u003e发现git默认生成的密钥对改成了ed25519，问了下AI如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003essh-keygen\u003c/code\u003e 是一个非常实用的命令行工具，用于生成 \u003cstrong\u003eSSH (Secure Shell)\u003c/strong\u003e 协议使用的密钥对。这个密钥对通常由一个\u003cstrong\u003e私钥\u003c/strong\u003e和一个\u003cstrong\u003e公钥\u003c/strong\u003e组成。它们是 SSH 安全通信的基石，允许用户在不传输密码的情况下安全地认证到远程服务器。\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"密钥对的核心概念\"\u003e密钥对的核心概念\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e公钥 (Public Key)：\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e这是您可以\u003cstrong\u003e安全地共享\u003c/strong\u003e给任何人的部分。\u003c/li\u003e\n\u003cli\u003e您需要将它放在您想要连接的远程服务器上（通常在 \u003ccode\u003e~/.ssh/authorized_keys\u003c/code\u003e 文件中）。\u003c/li\u003e\n\u003cli\u003e公钥用于\u003cstrong\u003e加密\u003c/strong\u003e数据，并且可以\u003cstrong\u003e验证\u003c/strong\u003e私钥签名的数据。\u003c/li\u003e\n\u003cli\u003e它不能用来解密数据或进行身份认证，只能用于验证私钥持有者的身份。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e私钥 (Private Key)：\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e这是您需要\u003cstrong\u003e严格保密\u003c/strong\u003e的部分。绝不能与任何人共享。\u003c/li\u003e\n\u003cli\u003e它通常存储在您的本地计算机上（例如 \u003ccode\u003e~/.ssh/id_rsa\u003c/code\u003e）。\u003c/li\u003e\n\u003cli\u003e私钥用于\u003cstrong\u003e解密\u003c/strong\u003e由对应公钥加密的数据，并用于向远程服务器\u003cstrong\u003e签名\u003c/strong\u003e认证请求。\u003c/li\u003e\n\u003cli\u003e如果私钥泄露，您的 SSH 连接安全就会受到威胁。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"ssh-keygen-的基本用法\"\u003e\u003ccode\u003essh-keygen\u003c/code\u003e 的基本用法\u003c/h3\u003e\n\u003cp\u003e最简单的生成命令是：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003essh-keygen\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e执行这个命令后，\u003ccode\u003essh-keygen\u003c/code\u003e 会引导您完成以下几个步骤：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e文件保存路径：\u003c/strong\u003e 它会提示您选择密钥对的保存位置。默认情况下，私钥会保存在 \u003ccode\u003e~/.ssh/id_rsa\u003c/code\u003e（对于 RSA 算法）或 \u003ccode\u003e~/.ssh/id_ed25519\u003c/code\u003e（对于 Ed25519 算法），公钥则会是相同路径加上 \u003ccode\u003e.pub\u003c/code\u003e 后缀（例如 \u003ccode\u003e~/.ssh/id_rsa.pub\u003c/code\u003e）。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eEnter file in which to save the key (/home/user/.ssh/id_rsa):\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e如果您不输入任何内容直接回车，就会使用默认路径。如果您想生成多个密钥对用于不同目的，可以输入不同的文件名。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e密码短语 (Passphrase)：\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e这是非常重要的一步。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e作用：\u003c/strong\u003e 为您的\u003cstrong\u003e私钥\u003c/strong\u003e加密。即使私钥文件被盗，没有密码短语也无法使用。这提供了额外的安全层。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e建议：\u003c/strong\u003e \u003cstrong\u003e强烈建议\u003c/strong\u003e设置一个强密码短语。虽然每次使用私钥时都需要输入密码短语会增加一点点不便，但它能极大地提升安全性。您可以使用 \u003ccode\u003essh-agent\u003c/code\u003e 来缓存密码短语，避免每次都输入。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e留空：\u003c/strong\u003e 如果您留空（不设置密码短语），私钥将不加密，这意味着任何获得该私钥文件的人都可以直接使用它。在自动化脚本等特定场景下可能会留空，但请务必了解其安全风险。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e密钥指纹和随机艺术图：\u003c/strong\u003e\n生成完成后，\u003ccode\u003essh-keygen\u003c/code\u003e 会显示密钥的\u003cstrong\u003e指纹 (fingerprint)\u003c/strong\u003e 和一个随机生成的\u003cstrong\u003e艺术图 (randomart image)\u003c/strong\u003e。这些信息用于视觉验证密钥的唯一性。\u003c/p\u003e","title":"k3s and wsl"},{"content":"近日举办的2025 RISC-V中国峰会上，NVIDIA硬件工程副总裁Frans Sijstermans宣布，CUDA软件平台将支持RISC-V指令集架构处理器，为开源架构RISC-V开启进入数据中心与AI市场的大门。\n遗憾的是，Go 的 C 语言 FFI 实在太差了。cgo 的糟糕程度怎么说都不为过。\n使用Gemini cli 尝试做一个自动抓取公司技术博客的应用。\nQwen-codeer 发布了，效果还可以，比Qwen好一些，但是有点贵\n调试百万分之一的故障：将 Pinterest 的搜索基础设施迁移到 Kubernetes，\n指标 container_referenced_bytes 在 cAdvisor 中默认启用，用于跟踪进程在每个测量周期中引用的总内存字节数。该指标通过 PTE 访问位机制实现了 Brendan Gregg 的工作集大小（WSS）估计，该机制使用页表中的访问位来计算进程读取或写入的总内存量。每次 cAdvisor 运行时，它会扫描整个页表来计算此统计数据，通过统计所有条目中的所有访问位，然后清除每个已访问的位。PinCompute 每 30 秒运行一次 cAdvisor，这意味着这种侵入性的访问位检查和清除每分钟发生两次，每分钟一次。\nManas 搜索索引每个叶节点可能高达数百 GB，而二级排序索引可能超过 1TB。当 Manas 叶节点启动时，它们会将整个索引映射到内存中。这意味着内存使用可能非常显著（例如，对于内存使用量为 100GB 的主机，页表可以容纳 2500 万个条目）。每 30 秒遍历并清除 2500 万个页表条目可能会确实导致与内存密集型叶处理产生竞争。\n","permalink":"https://vsxen.github.io/posts/2025-07-27/","summary":"\u003cp\u003e近日举办的2025 RISC-V中国峰会上，NVIDIA硬件工程副总裁Frans Sijstermans宣布，CUDA软件平台将支持RISC-V指令集架构处理器，为开源架构RISC-V开启进入数据中心与AI市场的大门。\u003c/p\u003e\n\u003cp\u003e遗憾的是，Go 的 C 语言 FFI 实在太差了。cgo 的糟糕程度怎么说都不为过。\u003c/p\u003e\n\u003cp\u003e使用Gemini cli 尝试做一个自动抓取公司技术博客的应用。\u003c/p\u003e\n\u003cp\u003eQwen-codeer 发布了，效果还可以，比Qwen好一些，但是有点贵\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://medium.com/pinterest-engineering/debugging-the-one-in-a-million-failure-migrating-pinterests-search-infrastructure-to-kubernetes-bef9af9dabf4\"\u003e调试百万分之一的故障：将 Pinterest 的搜索基础设施迁移到 Kubernetes\u003c/a\u003e，\u003c/p\u003e\n\u003cp\u003e指标 container_referenced_bytes 在 cAdvisor 中默认启用，用于跟踪进程在每个测量周期中引用的总内存字节数。该指标通过 PTE 访问位机制实现了 Brendan Gregg 的工作集大小（WSS）估计，该机制使用页表中的访问位来计算进程读取或写入的总内存量。每次 cAdvisor 运行时，它会扫描整个页表来计算此统计数据，通过统计所有条目中的所有访问位，然后清除每个已访问的位。PinCompute 每 30 秒运行一次 cAdvisor，这意味着这种侵入性的访问位检查和清除每分钟发生两次，每分钟一次。\u003c/p\u003e\n\u003cp\u003eManas 搜索索引每个叶节点可能高达数百 GB，而二级排序索引可能超过 1TB。当 Manas 叶节点启动时，它们会将整个索引映射到内存中。这意味着内存使用可能非常显著（例如，对于内存使用量为 100GB 的主机，页表可以容纳 2500 万个条目）。每 30 秒遍历并清除 2500 万个页表条目可能会确实导致与内存密集型叶处理产生竞争。\u003c/p\u003e","title":"CUDA 将支持RISC-V"},{"content":"Kimi K2 是一款具备更强代码能力、更擅长通用 Agent 任务的 MoE 架构基础模型，总参数 1T，激活参数 32B。\n在 SWE Bench Verified、Tau2、AceBench 等基准性能测试中，Kimi K2 均取得开源模型中的 SOTA 成绩，展现出在代码、Agent、数学推理任务上的领先能力。\n知乎提问\n在openrouter看到了groq，瞄了一下他们居然有自己的GPU。\n不过最近，GPU 的地位也在经受挑战：一家名为 Groq 的初创公司开发出了一种新的 AI 处理器 ——LPU（Language Processing Unit），其推理速度相较于英伟达 GPU 提高了 10 倍，成本却降低到十分之一。贾扬清在推特上算了一笔账，因为Groq小的可怜的内存容量（230MB），在运行Llama-2 70b模型时，需要305张Groq卡才足够，而用H100则只需要8张卡。从目前的价格来看，这意味着在同等吞吐量下，Groq的硬件成本是H100的40倍，能耗成本是10倍。\nApple CPU的推理框架要支持CUDA了\nEKS 100k node，之前k8s官方说最大5k节点，openai说他们有10k节点，这篇文的写的不错，都是之前遇到的问题。\n和上文对应，介绍如何保障APIserver的稳定\nopencode 好像火起来了，golang加ts写的。\nNFD 还可以探测操作系统的相关配置。\n谷歌DeepMind最近从OpenAI的潜在收购目标中挖走了Windsurf公司的核心团队，进一步增强了谷歌的AI技术实力，背后离不开戴密斯·哈萨比斯的操盘。\n","permalink":"https://vsxen.github.io/posts/2025-07-20/","summary":"\u003cp\u003eKimi K2 是一款具备更强代码能力、更擅长通用 Agent 任务的 MoE 架构基础模型，总参数 1T，激活参数 32B。\u003c/p\u003e\n\u003cp\u003e在 SWE Bench Verified、Tau2、AceBench 等基准性能测试中，Kimi K2 均取得开源模型中的 SOTA 成绩，展现出在代码、Agent、数学推理任务上的领先能力。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.zhihu.com/question/1927140506573435010\"\u003e知乎提问\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e在openrouter看到了groq，瞄了一下他们居然有自己的GPU。\u003c/p\u003e\n\u003cp\u003e不过最近，GPU 的地位也在经受挑战：一家名为 Groq 的初创公司开发出了一种新的 AI 处理器 ——LPU（Language Processing Unit），其推理速度相较于英伟达 GPU 提高了 10 倍，成本却降低到十分之一。贾扬清在推特上算了一笔账，因为Groq小的可怜的内存容量（230MB），在运行Llama-2 70b模型时，需要305张Groq卡才足够，而用H100则只需要8张卡。从目前的价格来看，这意味着在同等吞吐量下，Groq的硬件成本是H100的40倍，能耗成本是10倍。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ml-explore/mlx/pull/1983\"\u003eApple CPU的推理框架要支持CUDA了\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://aws.amazon.com/cn/blogs/containers/under-the-hood-amazon-eks-ultra-scale-clusters/\"\u003eEKS 100k node\u003c/a\u003e，之前k8s官方说最大5k节点，openai说他们有10k节点，这篇文的写的不错，都是之前遇到的问题。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ahmet.im/blog/kubernetes-list-performance/\"\u003e和上文对应，介绍如何保障APIserver的稳定\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eopencode 好像火起来了，golang加ts写的。\u003c/p\u003e\n\u003cp\u003eNFD 还可以探测操作系统的相关配置。\u003c/p\u003e\n\u003cp\u003e谷歌DeepMind最近从OpenAI的潜在收购目标中挖走了Windsurf公司的核心团队，进一步增强了谷歌的AI技术实力，背后离不开戴密斯·哈萨比斯的操盘。\u003c/p\u003e","title":"kimi k2 发布"},{"content":"那么HBM内存究竟是什么，与现在主流的LPDDR5有何不同呢？事实上，HBM（High Bandwidth Memory，高带宽内存）是一种基于3D堆栈技术，通过TSV（硅通孔）和微凸块（ubump）工艺实现的多层DRAM芯片垂直堆叠。而DDR（Double Data Rate）则采用的是并行总线架构，常见于DIMM（双列直插式内存模块）形式。\nHBM的带宽究竟有多高呢？以最新的HBM3E为例，其传输速率达到了9.6GB/s，可提供1.2TB/s带宽。作为对比，LPDDR5X的带宽为8533Mbps（1066.6MB/s），也就是说HBM3E的带宽是LPDDR5X的1180倍。\n这里\n目前只有kubelet支持动态修改log level。\n用于LLM的沙盒环境，没想到背后是依赖libkrun，containers组件下面的，后来看下了RedHat也有相关介绍。\n和集团那边沟通他们声称设计的推理集群最多只能支持32物理机节点，啊这有点不科学，天天都在听说什么万卡集群，这差的有点多吧。\n液冷服务器\n生产环境中的 LLM 推理， 这系列文章写的不错。\n试用了下perplexity，感觉一般，没有很惊讶。\nGORK 4 发布了。\n生活 已经提了离职，周五晚上到下周一去了趟青岛。\n电力如何调度的？这几天新闻说用电量一直达到历史新高。\n","permalink":"https://vsxen.github.io/posts/2025-07-13/","summary":"\u003cp\u003e那么HBM内存究竟是什么，与现在主流的LPDDR5有何不同呢？事实上，HBM（High Bandwidth Memory，高带宽内存）是一种基于3D堆栈技术，通过TSV（硅通孔）和微凸块（ubump）工艺实现的多层DRAM芯片垂直堆叠。而DDR（Double Data Rate）则采用的是并行总线架构，常见于DIMM（双列直插式内存模块）形式。\u003c/p\u003e\n\u003cp\u003eHBM的带宽究竟有多高呢？以最新的HBM3E为例，其传输速率达到了9.6GB/s，可提供1.2TB/s带宽。作为对比，LPDDR5X的带宽为8533Mbps（1066.6MB/s），也就是说HBM3E的带宽是LPDDR5X的1180倍。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://36kr.com/p/3365872192243719\"\u003e这里\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e目前只有kubelet支持动态修改log level。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/microsandbox/microsandbox/\"\u003e用于LLM的沙盒环境\u003c/a\u003e，没想到背后是依赖libkrun，\u003ca href=\"http://github.com/containers\"\u003econtainers\u003c/a\u003e组件下面的，后来看下了\u003ca href=\"https://developers.redhat.com/articles/2025/07/02/supercharging-ai-isolation-microvms-ramalama-libkrun#current_limitations_and_future_directions__gpu_enablement\"\u003eRedHat\u003c/a\u003e也有相关介绍。\u003c/p\u003e\n\u003cp\u003e和集团那边沟通他们声称设计的推理集群最多只能支持32物理机节点，啊这有点不科学，天天都在听说什么万卡集群，这差的有点多吧。\u003c/p\u003e\n\u003cp\u003e液冷服务器\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bentoml.com/llm/inference-optimization/data-tensor-pipeline-expert-hybrid-parallelism\"\u003e生产环境中的 LLM 推理\u003c/a\u003e， 这系列文章写的不错。\u003c/p\u003e\n\u003cp\u003e试用了下perplexity，感觉一般，没有很惊讶。\u003c/p\u003e\n\u003cp\u003eGORK 4 发布了。\u003c/p\u003e\n\u003ch2 id=\"生活\"\u003e生活\u003c/h2\u003e\n\u003cp\u003e已经提了离职，周五晚上到下周一去了趟青岛。\u003c/p\u003e\n\u003cp\u003e电力如何调度的？这几天新闻说用电量一直达到历史新高。\u003c/p\u003e","title":"故地重游与观海"},{"content":"体验过chatgpt，豆包，Qwen，和Gemini的深度研究之后，个人感觉Gemini是最好的。 首先会生成研究方向，搜索关键字，然后可能是通过tool call的方式抓取相关内容，生成的结果可以用来创建到Web Page（可交互），Infographic，Quiz和音频。\n当 kube-apiserver 处理 LIST 请求时，它会一次性将数据序列化为 JSON 或 Protobuf 格式，然后交由底层的 Go/http 处理。根据标准的 encoding/json 库实现，kube-apiserver 需要分配一大块内存来存放完整的序列化结果。更严重的是，这块内存要等到数据的最后一个字节被传输完毕后才会释放，容易导致高峰时的内存占用激增。\n解决这个问题的关键在于引入 流式处理 来序列化数据。KEP-5116 根据 LIST 响应的结构特点，可以依次序列化 TypeMeta、ListMeta，然后逐项序列化 Items，避免一次性分配和持有大块内存，从而降低内存占用。\nStreaming JSON/Protobuf in Kubernetes\nk8s 中 prometheus的 token 是如何访问经过认证的kube-controller-manager的/metrics的？\n答案是：证书验证（TLS）和Token授权（RBAC）是两个独立但连续的步骤。kube-controller-manager自己处理TLS握手，但会将授权决策委托给kube-apiserver。\n认证 (Authentication) - 使用 TokenReview：\nkube-controller-manager接收到Token后，它会创建一个TokenReview对象。这个对象包含了它从Prometheus收到的原始Token。 它向kube-apiserver的/apis/authentication.k8s.io/v1/tokenreviews端点发起一个请求，内容就是这个TokenReview对象。 kube-apiserver收到请求后，会验证这个Token的签名和有效性。如果Token有效，kube-apiserver会在TokenReview对象的状态（status）字段中填充该Token对应的用户信息（用户名、UID、所属组等），并返回给kube-controller-manager。 现在，kube-controller-manager知道了这个请求的发起者是谁（例如，system:serviceaccount:monitoring:prometheus-k8s）。 授权 (Authorization) - 使用 SubjectAccessReview：\n在知道了请求者的身份后，kube-controller-manager需要确认这个身份是否有权限执行请求的操作（即对/metrics路径进行GET操作）。 它会创建一个SubjectAccessReview对象。这个对象里包含了上一步获取到的用户信息以及本次请求试图执行的操作（verb: \u0026quot;get\u0026quot;, nonResourceURL: \u0026quot;/metrics\u0026quot;）。 它向kube-apiserver的/apis/authorization.k8s.io/v1/subjectaccessreviews端点发起请求。 kube-apiserver收到请求后，会查询集群中所有的RBAC规则（Role, ClusterRole, RoleBinding, ClusterRoleBinding），判断这个用户是否有权限执行该操作。 kube-apiserver将检查结果（允许或拒绝）填充到SubjectAccessReview对象的状态字段中，并返回给kube-controller-manager。 最终决策：\nkube-controller-manager收到SubjectAccessReview的响应后，如果结果是“允许”，它就会向Prometheus返回/metrics的数据。 如果结果是“拒绝”，它会向Prometheus返回403 Forbidden错误。 这保证了整个集群的认证授权策略是统一和集中的，避免了每个组件各自为政带来的安全风险和管理复杂性。\nk8s 为什么要设计service/proxy的子资源?\nAI生成了4个理由，我觉得唯一有点说服力的也就第三条。\n提供稳定的编程接口（Programmatic Access） 通过提供一个标准的 RESTful API 端点（GET /api/v1/namespaces/{namespace}/services/{service-name}:{port-name}/proxy/{path}），Kubernetes 为自动化工具和自定义控制器（Controller）提供了一个与内部服务交互的稳定接口。 一个经典案例就是 Kubernetes 早期的监控组件 Heapster（现在已被 Metrics Server 取代），它就是通过 service/proxy 接口去轮询各个组件（如 Kubelet）的 metrics 端点来采集监控数据的。这使得集群扩展组件的开发变得更加简单和标准化。\n精细化的权限控制（RBAC） 简化开发与调试工作 符合逻辑的 RESTful API 设计 腾讯云的账号体系太乱了。\n腾讯云的云开发，这配置有点高，128线程CPU，还是非NUMA结构：\n➜ QwQ-32B free -mh total used free shared buff/cache available Mem: 128Gi 3.1Gi 123Gi 0B 1.3Gi 124Gi Swap: 0B 0B 0B ➜ QwQ-32B lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Address sizes: 52 bits physical, 48 bits virtual Byte Order: Little Endian CPU(s): 128 On-line CPU(s) list: 0-127 Vendor ID: AuthenticAMD Model name: AMD EPYC 9754 128-Core Processor CPU family: 25 Model: 160 Thread(s) per core: 2 Core(s) per socket: 64 Socket(s): 1 Stepping: 2 BogoMIPS: 4500.04 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good no pl nonstop_tsc cpuid extd_apicid amd_dcm tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single ibpb vmmcall fsgsbase tsc_adjust bmi1 avx2 sm ep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 avx512_bf16 clzero xsaveerptr w bnoinvd arat avx512vbmi umip avx512_vbmi2 vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm Virtualization features: Hypervisor vendor: KVM Virtualization type: full Caches (sum of all): L1d: 2 MiB (64 instances) L1i: 2 MiB (64 instances) L2: 64 MiB (64 instances) L3: 128 MiB (8 instances) NUMA: NUMA node(s): 1 NUMA node0 CPU(s): 0-127 Vulnerabilities: Itlb multihit: Not affected L1tf: Not affected Mds: Not affected Meltdown: Not affected Mmio stale data: Not affected Retbleed: Not affected Spec store bypass: Vulnerable Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization Spectre v2: Mitigation; Retpolines, IBPB conditional, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected Srbds: Not affected Tsx async abort: Not affected Android 那边开源trace工具，抓了30秒，好像没啥数据，Chrome也没有抓到，当然也支持了Linux（之前还需要手动编译）。\n生活 人生开门红挺好看的，喜剧感满满，最后的结局也是挺讽刺的。\n天气好热，17点前根本不能出门。\n","permalink":"https://vsxen.github.io/posts/2025-07-06/","summary":"\u003cp\u003e体验过chatgpt，豆包，Qwen，和Gemini的深度研究之后，个人感觉Gemini是最好的。\n首先会生成研究方向，搜索关键字，然后可能是通过tool call的方式抓取相关内容，生成的结果可以用来创建到Web Page（可交互），Infographic，Quiz和音频。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"../../images/deep-research.png\"\u003e\u003c/p\u003e\n\u003cp\u003e当 kube-apiserver 处理 LIST 请求时，它会一次性将数据序列化为 JSON 或 Protobuf 格式，然后交由底层的 Go/http 处理。根据标准的 encoding/json 库实现，kube-apiserver 需要分配一大块内存来存放完整的序列化结果。更严重的是，这块内存要等到数据的最后一个字节被传输完毕后才会释放，容易导致高峰时的内存占用激增。\u003c/p\u003e\n\u003cp\u003e解决这个问题的关键在于引入 流式处理 来序列化数据。KEP-5116 根据 LIST 响应的结构特点，可以依次序列化 TypeMeta、ListMeta，然后逐项序列化 Items，避免一次性分配和持有大块内存，从而降低内存占用。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://fuweid.com/post/2025-streaming-jsonpb-in-k8s/\"\u003eStreaming JSON/Protobuf in Kubernetes\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003ek8s 中 prometheus的 token 是如何访问经过认证的kube-controller-manager的/metrics的？\u003c/p\u003e\n\u003cp\u003e答案是：\u003cstrong\u003e证书验证（TLS）和Token授权（RBAC）是两个独立但连续的步骤。\u003ccode\u003ekube-controller-manager\u003c/code\u003e自己处理TLS握手，但会将授权决策委托\u003c/strong\u003e给\u003ccode\u003ekube-apiserver\u003c/code\u003e。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e认证 (Authentication) - 使用 \u003ccode\u003eTokenReview\u003c/code\u003e\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekube-controller-manager\u003c/code\u003e接收到Token后，它会创建一个\u003ccode\u003eTokenReview\u003c/code\u003e对象。这个对象包含了它从Prometheus收到的原始Token。\u003c/li\u003e\n\u003cli\u003e它向\u003ccode\u003ekube-apiserver\u003c/code\u003e的\u003ccode\u003e/apis/authentication.k8s.io/v1/tokenreviews\u003c/code\u003e端点发起一个请求，内容就是这个\u003ccode\u003eTokenReview\u003c/code\u003e对象。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekube-apiserver\u003c/code\u003e收到请求后，会验证这个Token的签名和有效性。如果Token有效，\u003ccode\u003ekube-apiserver\u003c/code\u003e会在\u003ccode\u003eTokenReview\u003c/code\u003e对象的状态（status）字段中填充该Token对应的用户信息（用户名、UID、所属组等），并返回给\u003ccode\u003ekube-controller-manager\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e现在，\u003ccode\u003ekube-controller-manager\u003c/code\u003e知道了这个请求的发起者是谁（例如，\u003ccode\u003esystem:serviceaccount:monitoring:prometheus-k8s\u003c/code\u003e）。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e授权 (Authorization) - 使用 \u003ccode\u003eSubjectAccessReview\u003c/code\u003e\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在知道了请求者的身份后，\u003ccode\u003ekube-controller-manager\u003c/code\u003e需要确认这个身份是否有权限执行请求的操作（即对\u003ccode\u003e/metrics\u003c/code\u003e路径进行\u003ccode\u003eGET\u003c/code\u003e操作）。\u003c/li\u003e\n\u003cli\u003e它会创建一个\u003ccode\u003eSubjectAccessReview\u003c/code\u003e对象。这个对象里包含了上一步获取到的用户信息以及本次请求试图执行的操作（\u003ccode\u003everb: \u0026quot;get\u0026quot;\u003c/code\u003e, \u003ccode\u003enonResourceURL: \u0026quot;/metrics\u0026quot;\u003c/code\u003e）。\u003c/li\u003e\n\u003cli\u003e它向\u003ccode\u003ekube-apiserver\u003c/code\u003e的\u003ccode\u003e/apis/authorization.k8s.io/v1/subjectaccessreviews\u003c/code\u003e端点发起请求。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekube-apiserver\u003c/code\u003e收到请求后，会查询集群中所有的RBAC规则（\u003ccode\u003eRole\u003c/code\u003e, \u003ccode\u003eClusterRole\u003c/code\u003e, \u003ccode\u003eRoleBinding\u003c/code\u003e, \u003ccode\u003eClusterRoleBinding\u003c/code\u003e），判断这个用户是否有权限执行该操作。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekube-apiserver\u003c/code\u003e将检查结果（允许或拒绝）填充到\u003ccode\u003eSubjectAccessReview\u003c/code\u003e对象的状态字段中，并返回给\u003ccode\u003ekube-controller-manager\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e最终决策\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekube-controller-manager\u003c/code\u003e收到\u003ccode\u003eSubjectAccessReview\u003c/code\u003e的响应后，如果结果是“允许”，它就会向Prometheus返回\u003ccode\u003e/metrics\u003c/code\u003e的数据。\u003c/li\u003e\n\u003cli\u003e如果结果是“拒绝”，它会向Prometheus返回\u003ccode\u003e403 Forbidden\u003c/code\u003e错误。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e这保证了整个集群的认证授权策略是统一和集中的，避免了每个组件各自为政带来的安全风险和管理复杂性。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003ek8s 为什么要设计service/proxy的子资源?\u003c/p\u003e\n\u003cp\u003eAI生成了4个理由，我觉得唯一有点说服力的也就第三条。\u003c/p\u003e","title":"Gemini Deep Research"},{"content":"简单用了下claude artifact，效果如下： 我是普通账号，上下文长度有点短，生成之后就不能做修改了，步骤分解第一次是自己生成，截图这次是手动补充的。 后端接口接口不能直接发布，只能作为纯文本预览。\n同样的提示词在aistduio上面用Gemini 2.5 pro效果就差了一些：\nhttps://github.com/vllm-project/production-stack vllm官方的最佳实践\nletsencrypt 准备颁发IP证书了\nGemini Cli 发布，可以免费用2.5 pro，超过以后会降到2.5 flash，还是比较良心的\nTensorFlow 现在是不是比不上pytorch了？\n曾几何时，TensorFlow是深度学习框架领域无可争议的王者。然而，近年来，战局已悄然改变。来自学术界和产业界的多方证据表明，由Meta AI（原Facebook AI Research）主导开发的PyTorch，在用户青睐度、社区活跃度和学术研究应用等多个关键指标上，已经超越了Google支持的TensorFlow。虽然TensorFlow在特定的工业生产环境中仍占有一席之地，但“PyTorch后来居上”已成为业界的普遍共识。\n学术界的压倒性优势与开发者的普遍偏爱 目前，PyTorch在学术研究领域的主导地位尤为突出。根据PyTorch官方在2024年底发布的回顾报告，超过70%的AI研究论文实现采用了PyTorch。这一数据得到了各大顶级AI会议论文代码实现的印证，PyTorch的出现频率远高于TensorFlow。这种趋势的背后，是PyTorch以其简洁、灵活和“Pythonic”的编程风格赢得了广大学者和开发者的心。\n工业界的版图变迁：从TensorFlow独大到两强并立 传统上，TensorFlow凭借其强大的生态系统，如用于模型部署的TensorFlow Serving、用于移动和嵌入式设备的TensorFlow Lite（TFLite）以及端到端机器学习平台TFX，在工业界，特别是大规模生产部署方面，占据了绝对优势。许多大型科技公司，包括Google自身，其内部大量的AI应用和系统都深度绑定了TensorFlow。\n然而，随着PyTorch的日渐成熟和其生态的不断完善，这一格局正在被打破。PyTorch在2.0版本后，通过引入torch.compile等功能，显著提升了训练性能，缩小了与TensorFlow在速度上的差距。同时，TorchServe等部署工具的推出，也补齐了其在生产环境中的短板。\n更重要的是，随着大量在校期间习惯使用PyTorch的学生和研究人员进入工业界，企业的新项目越来越倾向于采用PyTorch。许多公司，特别是那些追求快速迭代和创新的AI初创企业，已将PyTorch作为首选框架。虽然让拥有庞大TensorFlow技术栈的公司进行“伤筋动骨”的迁移尚不现实，但在新项目的选择上，天平已明显倾斜。\nHugging Face生态的“风向标”意义 作为全球最大的AI模型和数据集社区，Hugging Face上模型的框架分布是衡量框架流行度的重要“风向标”。尽管没有精确的官方统计数据持续发布，但社区的普遍观察和模型上传趋势显示，绝大多数最新的、SOTA（State-of-the-Art）的自然语言处理（NLP）模型，尤其是大语言模型（LLMs），都优先提供PyTorch版本。这得益于Hugging Face的Transformers库与PyTorch的无缝集成。开发者可以轻松地使用PyTorch对Hugging Face上的模型进行微调和再训练，这极大地促进了PyTorch在NLP领域的统治地位。\n那什么又是 Pythonic 呢？\ncline 可以从Gemini cli中调用2.5 pro的接口的，后续又去掉了，哈哈。\nhttps://github.com/musistudio/claude-code-router 想用claude code但是没有合适的购买途径，就可以用这个项目。\n生活 bilibili首页刷到了某豪车租赁的账号。\n为啥AMD突然就把英特尔干趴了？Intel的股价快到历史新低了。\n","permalink":"https://vsxen.github.io/posts/2025-06-29/","summary":"\u003cp\u003e简单用了下claude artifact，效果如下：\n\u003cimg loading=\"lazy\" src=\"../../images/claude-artifact.png\"\u003e\n我是普通账号，上下文长度有点短，生成之后就不能做修改了，步骤分解第一次是自己生成，截图这次是手动补充的。\n后端接口接口不能直接发布，只能作为纯文本预览。\u003c/p\u003e\n\u003cp\u003e同样的提示词在aistduio上面用Gemini 2.5 pro效果就差了一些：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"../../images/aistduio-build.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/vllm-project/production-stack\"\u003ehttps://github.com/vllm-project/production-stack\u003c/a\u003e vllm官方的最佳实践\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://letsencrypt.org/2025/07/01/issuing-our-first-ip-address-certificate/\"\u003eletsencrypt 准备颁发IP证书了\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGemini Cli 发布，可以免费用2.5 pro，超过以后会降到2.5 flash，还是比较良心的\u003c/p\u003e\n\u003cp\u003eTensorFlow 现在是不是比不上pytorch了？\u003c/p\u003e\n\u003cp\u003e曾几何时，TensorFlow是深度学习框架领域无可争议的王者。然而，近年来，战局已悄然改变。来自学术界和产业界的多方证据表明，由Meta AI（原Facebook AI Research）主导开发的PyTorch，在用户青睐度、社区活跃度和学术研究应用等多个关键指标上，已经超越了Google支持的TensorFlow。虽然TensorFlow在特定的工业生产环境中仍占有一席之地，但“PyTorch后来居上”已成为业界的普遍共识。\u003c/p\u003e\n\u003cp\u003e学术界的压倒性优势与开发者的普遍偏爱\n目前，PyTorch在学术研究领域的主导地位尤为突出。根据PyTorch官方在2024年底发布的回顾报告，超过70%的AI研究论文实现采用了PyTorch。这一数据得到了各大顶级AI会议论文代码实现的印证，PyTorch的出现频率远高于TensorFlow。这种趋势的背后，是PyTorch以其简洁、灵活和“Pythonic”的编程风格赢得了广大学者和开发者的心。\u003c/p\u003e\n\u003cp\u003e工业界的版图变迁：从TensorFlow独大到两强并立\n传统上，TensorFlow凭借其强大的生态系统，如用于模型部署的TensorFlow Serving、用于移动和嵌入式设备的TensorFlow Lite（TFLite）以及端到端机器学习平台TFX，在工业界，特别是大规模生产部署方面，占据了绝对优势。许多大型科技公司，包括Google自身，其内部大量的AI应用和系统都深度绑定了TensorFlow。\u003c/p\u003e\n\u003cp\u003e然而，随着PyTorch的日渐成熟和其生态的不断完善，这一格局正在被打破。PyTorch在2.0版本后，通过引入torch.compile等功能，显著提升了训练性能，缩小了与TensorFlow在速度上的差距。同时，TorchServe等部署工具的推出，也补齐了其在生产环境中的短板。\u003c/p\u003e\n\u003cp\u003e更重要的是，随着大量在校期间习惯使用PyTorch的学生和研究人员进入工业界，企业的新项目越来越倾向于采用PyTorch。许多公司，特别是那些追求快速迭代和创新的AI初创企业，已将PyTorch作为首选框架。虽然让拥有庞大TensorFlow技术栈的公司进行“伤筋动骨”的迁移尚不现实，但在新项目的选择上，天平已明显倾斜。\u003c/p\u003e\n\u003cp\u003eHugging Face生态的“风向标”意义\n作为全球最大的AI模型和数据集社区，Hugging Face上模型的框架分布是衡量框架流行度的重要“风向标”。尽管没有精确的官方统计数据持续发布，但社区的普遍观察和模型上传趋势显示，绝大多数最新的、SOTA（State-of-the-Art）的自然语言处理（NLP）模型，尤其是大语言模型（LLMs），都优先提供PyTorch版本。这得益于Hugging Face的Transformers库与PyTorch的无缝集成。开发者可以轻松地使用PyTorch对Hugging Face上的模型进行微调和再训练，这极大地促进了PyTorch在NLP领域的统治地位。\u003c/p\u003e\n\u003cp\u003e那什么又是 Pythonic 呢？\u003c/p\u003e\n\u003cp\u003ecline 可以从Gemini cli中调用2.5 pro的接口的，后续又去掉了，哈哈。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/musistudio/claude-code-router\"\u003ehttps://github.com/musistudio/claude-code-router\u003c/a\u003e 想用claude code但是没有合适的购买途径，就可以用这个项目。\u003c/p\u003e\n\u003ch2 id=\"生活\"\u003e生活\u003c/h2\u003e\n\u003cp\u003ebilibili首页刷到了某豪车租赁的账号。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.bilibili.com/video/BV1R67GzsEQf/\"\u003e为啥AMD突然就把英特尔干趴了？\u003c/a\u003eIntel的股价快到历史新低了。\u003c/p\u003e","title":"Artifact vs Canvas"},{"content":"DB-GPT 是真的一般呀，UI一般，经常还有报错。\nUsing AI for Troubleshooting: OpenAI vs DeepSeek Kaniko 终于正式归档了，构建工具用哪一个呢？\nGemini Code Assist VSCode插件发布了，速度好慢。\nGPUmanager 分析，已经不维护了，后续推荐https://github.com/Project-HAMi。\nopenrouter 还有一个free的模型。\n都说ragflow的rag功能比dify要好，测试了一下。\n生活 腾讯云智面试，字节面试，问了一个我觉得比较有意思的是问题：你对现在的架构最不满意/最想改进的地方是什么？ 我回答的是集群要跨AZ。\n离线下载音乐，想听歌越来越难了\n存款利率迈入“1”时代，\n","permalink":"https://vsxen.github.io/posts/2025-06-22/","summary":"\u003cp\u003eDB-GPT 是真的一般呀，UI一般，经常还有报错。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://coroot.com/blog/engineering/using-ai-for-troubleshooting-openai-vs-deepseek/\"\u003eUsing AI for Troubleshooting: OpenAI vs DeepSeek \u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eKaniko 终于正式归档了，构建工具用哪一个呢？\u003c/p\u003e\n\u003cp\u003eGemini Code Assist VSCode插件发布了，速度好慢。\u003c/p\u003e\n\u003cp\u003eGPUmanager 分析，已经不维护了，后续推荐https://github.com/Project-HAMi。\u003c/p\u003e\n\u003cp\u003eopenrouter 还有一个free的模型。\u003c/p\u003e\n\u003cp\u003e都说ragflow的rag功能比dify要好，测试了一下。\u003c/p\u003e\n\u003ch2 id=\"生活\"\u003e生活\u003c/h2\u003e\n\u003cp\u003e腾讯云智面试，字节面试，问了一个我觉得比较有意思的是问题：你对现在的架构最不满意/最想改进的地方是什么？\n我回答的是集群要跨AZ。\u003c/p\u003e\n\u003cp\u003e离线下载音乐，想听歌越来越难了\u003c/p\u003e\n\u003cp\u003e存款利率迈入“1”时代，\u003c/p\u003e","title":"如何离线下载音乐？"},{"content":"Claude Code 内部工作原理窥探 写的很不错，里面还有一篇关于LLM思考的文章。\n引申出了另一个问题，Proxy和这种代理的对比：\n特性 普通隧道代理 (Tunneling Proxy) 解密/拦截代理 (Intercepting Proxy) 能否看到明文 ❌ 不能 ✅ 可以 工作模式 HTTP CONNECT 隧道模式 中间人攻击 (MITM) 模式 与服务器的连接 客户端与服务器端到端加密 代理分别与客户端和服务器建立加密连接 是否需要安装根证书 否 是 (关键步骤) 主要用途 访问控制、IP 伪装、网络路由 网络调试、安全审计、数据修改 典型例子 多数商业 VPN 或普通网络代理 Mitmproxy, Burp Suite, Charles 这周是WWDC，发布了Apple Container，去年发布了 Hyper Framework，嗯macOS 开始成为最好的开发系统Apple Container，看了下issue。还有MTU的问题。\niOS 26 \u0026amp; Android 16 想整合多个vllm实例，搜索以后发现这个项目litellm。\n试用了Trae\nkube-proxy 的 nft 后端还是自己封装的？\n浑元3D\n","permalink":"https://vsxen.github.io/posts/2025-06-15/","summary":"\u003cp\u003e\u003ca href=\"https://xxchan.me/ai/2025/05/06/claude-code.html\"\u003eClaude Code 内部工作原理窥探\u003c/a\u003e\n写的很不错，里面还有一篇关于LLM思考的文章。\u003c/p\u003e\n\u003cp\u003e引申出了另一个问题，Proxy和这种代理的对比：\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003e特性\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003e普通隧道代理 (Tunneling Proxy)\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003e解密/拦截代理 (Intercepting Proxy)\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e能否看到明文\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e❌ \u003cstrong\u003e不能\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e✅ \u003cstrong\u003e可以\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e工作模式\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003ccode\u003eHTTP CONNECT\u003c/code\u003e 隧道模式\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e中间人攻击 (MITM) 模式\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e与服务器的连接\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e客户端与服务器端到端加密\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e代理分别与客户端和服务器建立加密连接\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e是否需要安装根证书\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e否\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e是 (关键步骤)\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e主要用途\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e访问控制、IP 伪装、网络路由\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e网络调试、安全审计、数据修改\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e典型例子\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e多数商业 VPN 或普通网络代理\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eMitmproxy, Burp Suite, Charles\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e这周是WWDC，发布了Apple Container，去年发布了 Hyper Framework，嗯macOS 开始成为最好的开发系统\u003ca href=\"https://github.com/apple/container\"\u003eApple Container\u003c/a\u003e，看了下issue。还有MTU的问题。\u003c/p\u003e\n\u003cp\u003eiOS 26 \u0026amp; \u003ca href=\"https://blog.google/products/android/android-16/\"\u003eAndroid 16\u003c/a\u003e\n想整合多个vllm实例，搜索以后发现这个项目\u003ca href=\"https://github.com/BerriAI/litellm\"\u003elitellm\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e试用了Trae\u003c/p\u003e\n\u003cp\u003ekube-proxy 的 nft 后端还是\u003ca href=\"https://github.com/kubernetes-sigs/knftables\"\u003e自己\u003c/a\u003e封装的？\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://3d.hunyuan.tencent.com/\"\u003e浑元3D\u003c/a\u003e\u003c/p\u003e","title":"Apple Container"},{"content":"codex 用rust重写了，之前是用的nodejs\n使用clickhouse作为后端存储的 open telemetry相关方案，有空得对比一下，https://github.com/hyperdxio/hyperdx 。\n[从etcd的数据文件导出原始数据](https://www.cncf.io/blog/2025/05/08/the-kubernetes-surgeons-handbook-precision-recovery-from-etcd-snap shots/)，比如原始yaml。\n看了下Cloudflare AI Gateway相关功能。\n从GUI中逆向出相关接口并转换为OPENAI格式\nDataherald已经不维护了，vanna只是框架，需要二次开发。\n使用 Gemini自建Deep research https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart\n特斯拉表示，Dojo 是当前全球唯二现存的最大处理器之一。这种晶圆级芯片采用整片 300mm 晶圆制成，单芯片尺寸已达物理极限。\n由于 Dojo 大芯片的超高复杂性，即使在制造过程中也难以 100% 检测缺陷晶粒，而静默数据错误的检测更困难。\n虽然 SDC 在所有硬件上都难以避免，但 Dojo 处理器有着 8,850 个核心、18000A 电流及 15000W 的超高功耗，这会严重放大其影响，因此所有核心必须按设计运行，否则单个数据错误便可毁掉整个耗时数周才能完成 AI 训练成果。\nhttps://x.com/Tesla_AI/status/1930686196201714027\nhttps://www.servethehome.com/tesla-dojo-exa-scale-lossy-ai-network-using-the-tesla-transport-protocol-over-ethernet-ttpoe/\n","permalink":"https://vsxen.github.io/posts/2025-06-08/","summary":"\u003cp\u003ecodex 用rust重写了，之前是用的nodejs\u003c/p\u003e\n\u003cp\u003e使用clickhouse作为后端存储的 open telemetry相关方案，有空得对比一下，https://github.com/hyperdxio/hyperdx 。\u003c/p\u003e\n\u003cp\u003e[从etcd的数据文件导出原始数据](\u003ca href=\"https://www.cncf.io/blog/2025/05/08/the-kubernetes-surgeons-handbook-precision-recovery-from-etcd-snap\"\u003ehttps://www.cncf.io/blog/2025/05/08/the-kubernetes-surgeons-handbook-precision-recovery-from-etcd-snap\u003c/a\u003e\nshots/)，比如原始yaml。\u003c/p\u003e\n\u003cp\u003e看了下Cloudflare AI Gateway相关功能。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/bincooo/chatgpt-adapter/discussions/106\"\u003e从GUI中逆向出相关接口并转换为OPENAI格式\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDataherald已经不维护了，vanna只是框架，需要二次开发。\u003c/p\u003e\n\u003cp\u003e使用 Gemini自建Deep research \u003ca href=\"https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart\"\u003ehttps://github.com/google-gemini/gemini-fullstack-langgraph-quickstart\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e特斯拉表示，Dojo 是当前全球唯二现存的最大处理器之一。这种晶圆级芯片采用整片 300mm 晶圆制成，单芯片尺寸已达物理极限。\u003c/p\u003e\n\u003cp\u003e由于 Dojo 大芯片的超高复杂性，即使在制造过程中也难以 100% 检测缺陷晶粒，而静默数据错误的检测更困难。\u003c/p\u003e\n\u003cp\u003e虽然 SDC 在所有硬件上都难以避免，但 Dojo 处理器有着 8,850 个核心、18000A 电流及 15000W 的超高功耗，这会严重放大其影响，因此所有核心必须按设计运行，否则单个数据错误便可毁掉整个耗时数周才能完成 AI 训练成果。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://x.com/Tesla_AI/status/1930686196201714027\"\u003ehttps://x.com/Tesla_AI/status/1930686196201714027\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.servethehome.com/tesla-dojo-exa-scale-lossy-ai-network-using-the-tesla-transport-protocol-over-ethernet-ttpoe/\"\u003ehttps://www.servethehome.com/tesla-dojo-exa-scale-lossy-ai-network-using-the-tesla-transport-protocol-over-ethernet-ttpoe/\u003c/a\u003e\u003c/p\u003e","title":"ChatDB和Text-to-SQL项目对比"},{"content":"想找一个AI Terminal，必须支持自定义API_BASE_URL，warp这个不行。\nData Formulator MS的开源项目，利用AI做数据可视化。\nclaude code发布了，和我之前思路类似，但是这个功能更丰富。\n查了下Gemini的API，主要是支持混合输出，当然也有兼容OPENAI的接口。\n通过硬件计数器，将性能提升3倍之旅\nDeepSeek 发布了R1的小版本更新0528\nhttps://blog.railway.com/p/data-center-build-part-two 裸金属上架\nhttps://github.com/google-gemini/gemini-fullstack-langgraph-quickstart\n","permalink":"https://vsxen.github.io/posts/2025-06-01/","summary":"\u003cp\u003e想找一个AI Terminal，必须支持自定义API_BASE_URL，\u003ca href=\"https://www.warp.dev\"\u003ewarp\u003c/a\u003e这个不行。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/microsoft/data-formulator\"\u003eData Formulator\u003c/a\u003e MS的开源项目，利用AI做数据可视化。\u003c/p\u003e\n\u003cp\u003eclaude code发布了，和我之前思路类似，但是这个功能更丰富。\u003c/p\u003e\n\u003cp\u003e查了下Gemini的API，主要是支持混合输出，当然也有兼容OPENAI的接口。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://cloud.tencent.com/developer/article/2168775\"\u003e通过硬件计数器，将性能提升3倍之旅\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDeepSeek 发布了R1的小版本更新0528\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://blog.railway.com/p/data-center-build-part-two\"\u003ehttps://blog.railway.com/p/data-center-build-part-two\u003c/a\u003e  裸金属上架\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart\"\u003ehttps://github.com/google-gemini/gemini-fullstack-langgraph-quickstart\u003c/a\u003e\u003c/p\u003e","title":"Claude Code"},{"content":"这周是Google I/O 2025，发布了很多东西，Jules这个后续测试了下，很慢一般般。\n微软 Build 2025：WSL 项目正式开源。\n看了deepflow的相关进展，支持了http2。\ngithub Trending 上面好多Agent的项目，没时间测试了。\n小红书mcp，根据小红书web的接口做的，前端有js加密，没想到直接用pyexecjs这种方式\n简单玩了下sora Engineering Everything with eBPF\n想试试发公众号了，看到了腾讯元器 和公众号打通了，但是没有坚持下来。\nhttps://nghiant3223.github.io/2025/04/15/go-scheduler.html\n","permalink":"https://vsxen.github.io/posts/2025-05-25/","summary":"\u003cp\u003e这周是Google I/O 2025，发布了很多东西，Jules这个后续测试了下，很慢一般般。\u003c/p\u003e\n\u003cp\u003e微软 Build 2025：WSL 项目正式开源。\u003c/p\u003e\n\u003cp\u003e看了deepflow的相关进展，支持了http2。\u003c/p\u003e\n\u003cp\u003egithub Trending 上面好多Agent的项目，没时间测试了。\u003c/p\u003e\n\u003cp\u003e小红书mcp，根据小红书web的接口做的，前端有js加密，没想到直接用pyexecjs这种方式\u003c/p\u003e\n\u003cp\u003e简单玩了下sora \u003cimg loading=\"lazy\" src=\"../../images/sora.webp\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ebpf.hamza-megahed.com/\"\u003eEngineering Everything with eBPF\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e想试试发公众号了，看到了\u003ca href=\"https://yuanqi.tencent.com/\"\u003e腾讯元器\u003c/a\u003e 和公众号打通了，但是没有坚持下来。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://nghiant3223.github.io/2025/04/15/go-scheduler.html\"\u003ehttps://nghiant3223.github.io/2025/04/15/go-scheduler.html\u003c/a\u003e\u003c/p\u003e","title":"Google I/O 2025 MS Build 2025"},{"content":"偶然发现github的console 有120+报错，怀疑是ublock的问题，怀疑是浏览器的问题，最后才发现是语雀浏览器插件导致的。\nGoogle launches Material 3 ，似乎已经没有之前那么惊艳了。\n看到open-telemetry准备支持arrow了，https://github.com/open-telemetry/otel-arrow 。 influxdb v3也正是这个发布了，告别了golang用rust重写，新的版本 。\ngrafana 的dashbard 要是支持AI就好了，识别异常波动进行告警，找了一下发现了这个。\nDeepseek 发了一个硬件架构相关的论文， Scaling Challenges and Reflections on Hardware for AI Architectures。\n看了下几个ebpf项目的进展，好像已经被AI掩盖过去了。\netcd 发布了3.6，也是好久没有更新了，还把etcdctl的部分操作拆除到etcdutl了，\nKubernetes v1.33 Fixes a 10-Year-Old Image Pull Loophole，有点夸张，就是多租户用到了同一个镜像，调度到同一个节点，kubelet就不会去验证镜像有没有拉取的权限。\n在微软任职 18 年的资深 TypeScript 工程师被裁员。\n大模型(LLM)驱动的APP自动化时代，使用xpath定位页面元素恐成为历史 ，思路不错，我之前是想找一个从页面中提取文字，然后总结的工具，用视觉模型（OCR）也可以，但是我还是喜欢文本。\nhttps://engineering.fb.com/2024/08/05/data-center-engineering/roce-network-distributed-ai-training-at-scale/\n生活 准备安装充电桩了，先看下怎么不被坑==\n想起来之前看的一个博主，Koala 聊开源。\n","permalink":"https://vsxen.github.io/posts/2025-05-18/","summary":"\u003cp\u003e偶然发现github的console 有120+报错，怀疑是ublock的问题，怀疑是浏览器的问题，最后才发现是语雀浏览器插件导致的。\u003c/p\u003e\n\u003cp\u003eGoogle launches Material 3 ，似乎已经没有之前那么惊艳了。\u003c/p\u003e\n\u003cp\u003e看到open-telemetry准备支持arrow了，https://github.com/open-telemetry/otel-arrow 。\ninfluxdb v3也正是这个发布了，告别了golang用rust重写，\u003ca href=\"https://docs.influxdata.com/influxdb3/core/get-started/\"\u003e新的版本\u003c/a\u003e 。\u003c/p\u003e\n\u003cp\u003egrafana 的dashbard 要是支持AI就好了，识别异常波动进行告警，找了一下发现了\u003ca href=\"https://github.com/grafana/grafana-llm-app\"\u003e这个\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003eDeepseek 发了一个硬件架构相关的论文， \u003ca href=\"https://www.arxiv.org/pdf/2505.09343\"\u003eScaling Challenges and Reflections on Hardware for AI Architectures\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e看了下几个ebpf项目的进展，好像已经被AI掩盖过去了。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://etcd.io/blog/2025/announcing-etcd-3.6/\"\u003eetcd 发布了3.6\u003c/a\u003e，也是好久没有更新了，还把etcdctl的部分操作拆除到etcdutl了，\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://blog.abhimanyu-saharan.com/posts/kubernetes-v1-33-fixes-a-10-year-old-image-pull-loophole\"\u003eKubernetes v1.33 Fixes a 10-Year-Old Image Pull Loophole\u003c/a\u003e，有点夸张，就是多租户用到了同一个镜像，调度到同一个节点，kubelet就不会去验证镜像有没有拉取的权限。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.oschina.net/news/350003/ms-lays-off-typescript-veteran-in-latest-job-cuts\"\u003e在微软任职 18 年的资深 TypeScript 工程师被裁员\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://hanzilu.com/wordpress/?p=429\"\u003e大模型(LLM)驱动的APP自动化时代，使用xpath定位页面元素恐成为历史 \u003c/a\u003e，思路不错，我之前是想找一个从页面中提取文字，然后总结的工具，用视觉模型（OCR）也可以，但是我还是喜欢文本。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://engineering.fb.com/2024/08/05/data-center-engineering/roce-network-distributed-ai-training-at-scale/\"\u003ehttps://engineering.fb.com/2024/08/05/data-center-engineering/roce-network-distributed-ai-training-at-scale/\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"生活\"\u003e生活\u003c/h2\u003e\n\u003cp\u003e准备安装充电桩了，先看下怎么不被坑==\u003c/p\u003e\n\u003cp\u003e想起来之前看的一个博主，Koala 聊开源。\u003c/p\u003e","title":"如何让LLM进入日产工作"}]